{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pilot Study v2\n",
    "\n",
    "Multiple experiments (from `TAGS`) scoring news articles on a 4-point fascism rubric.\n",
    "\n",
    "Data is pulled via `judge_gym.collect.pull_experiments` (single bulk Convex query per experiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize as mplNormalize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from itertools import combinations\n",
    "from functools import reduce\n",
    "from pyds import MassFunction\n",
    "\n",
    "from judge_gym.collect import pull_experiments\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAGS = [\n",
    "#     \"ecc-fascism-usa-trial-gpt-4.1\",\n",
    "#     \"ecc-fascism-usa-trial-gemini-3.0-flash\",\n",
    "#     \"ecc-fascism-usa-trial-gpt-5.2-chat\",\n",
    "#     \"ecc-fascism-usa-trial-qwen3-235b\",\n",
    "#     \"ecc-fascism-usa-trial-gpt-4.1-mini\",\n",
    "# ]\n",
    "\n",
    "# data = pull_experiments(TAGS)\n",
    "# print(f\"Tags pulled: {data.tags}\")\n",
    "# print(f\"Scale size:  {data.scale_size}\")\n",
    "# print(f\"Scores:      {len(data.scores)} rows\")\n",
    "# print(f\"Evidence:    {len(data.evidence)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"https://raw.githubusercontent.com/theonetruejesse/pilot-v2-data/main/experiments.csv\"\n",
    "TAGS = [\n",
    "    \"ecc-fascism-usa-trial-gpt-4.1\",\n",
    "    \"ecc-fascism-usa-trial-gemini-3.0-flash\",\n",
    "    \"ecc-fascism-usa-trial-gpt-5.2-chat\",\n",
    "    \"ecc-fascism-usa-trial-qwen3-235b\",\n",
    "]\n",
    "\n",
    "\n",
    "def _maybe_parse_json(val: Any) -> Any:\n",
    "    if not isinstance(val, str):\n",
    "        return val\n",
    "    s = val.strip()\n",
    "    if not s or s[0] not in \"[{\":\n",
    "        return val\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except json.JSONDecodeError:\n",
    "        return val\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentData:\n",
    "    scores: pd.DataFrame\n",
    "    evidence: pd.DataFrame\n",
    "    rubrics: pd.DataFrame\n",
    "    experiments: dict[str, dict[str, Any]]\n",
    "    tags: list[str]\n",
    "\n",
    "    @property\n",
    "    def scale_size(self) -> int:\n",
    "        return int(self.scores[\"scaleSize\"].dropna().iloc[0])\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "for col in [\"decodedScores\", \"rubric.qualityStats\", \"rubric.stages\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(_maybe_parse_json)\n",
    "\n",
    "# Scores\n",
    "scores = df.copy()\n",
    "\n",
    "# Evidence\n",
    "if {\"evidenceId\", \"evidenceLabel\", \"evidenceTitle\"}.issubset(df.columns):\n",
    "    evidence = (\n",
    "        df[[\"evidenceId\", \"evidenceLabel\", \"evidenceTitle\"]]\n",
    "        .drop_duplicates(subset=\"evidenceId\")\n",
    "        .rename(columns={\"evidenceLabel\": \"label\", \"evidenceTitle\": \"title\"})\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    evidence = pd.DataFrame(columns=[\"evidenceId\", \"label\", \"title\"])\n",
    "\n",
    "# Rubrics\n",
    "if {\"rubricId\", \"rubric.qualityStats\", \"rubric.stages\"}.issubset(df.columns):\n",
    "    rubrics = (\n",
    "        df[[\"rubricId\", \"rubric.qualityStats\", \"rubric.stages\"]]\n",
    "        .drop_duplicates(subset=\"rubricId\")\n",
    "        .rename(columns={\"rubric.qualityStats\": \"qualityStats\", \"rubric.stages\": \"stages\"})\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "else:\n",
    "    rubrics = pd.DataFrame(columns=[\"rubricId\", \"qualityStats\", \"stages\"])\n",
    "\n",
    "# Experiments (only modelId needed by notebook)\n",
    "experiments = {}\n",
    "if {\"experimentTag\", \"experiment.modelId\"}.issubset(df.columns):\n",
    "    exp_df = (\n",
    "        df[[\"experimentTag\", \"experiment.modelId\"]]\n",
    "        .drop_duplicates(subset=\"experimentTag\")\n",
    "        .rename(columns={\"experiment.modelId\": \"modelId\"})\n",
    "    )\n",
    "    experiments = exp_df.set_index(\"experimentTag\").to_dict(orient=\"index\")\n",
    "\n",
    "# Tags (preserve TAGS order if present)\n",
    "found_tags = [t for t in TAGS if t in set(df[\"experimentTag\"].unique())]\n",
    "if not found_tags:\n",
    "    found_tags = sorted(df[\"experimentTag\"].unique().tolist())\n",
    "\n",
    "# Filter to TAGS if provided\n",
    "scores = scores[scores[\"experimentTag\"].isin(found_tags)].copy()\n",
    "\n",
    "# Final container\n",
    "data = ExperimentData(\n",
    "    scores=scores,\n",
    "    evidence=evidence,\n",
    "    rubrics=rubrics,\n",
    "    experiments=experiments,\n",
    "    tags=found_tags,\n",
    ")\n",
    "\n",
    "print(f\"Tags pulled: {data.tags}\")\n",
    "print(f\"Scale size:  {data.scale_size}\")\n",
    "print(f\"Scores:      {len(data.scores)} rows\")\n",
    "print(f\"Evidence:    {len(data.evidence)} articles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = data.scores.copy()\n",
    "eid_to_label = dict(zip(data.evidence[\"evidenceId\"], data.evidence[\"label\"]))\n",
    "scores[\"evidence\"] = scores[\"evidenceId\"].map(eid_to_label)\n",
    "\n",
    "data.evidence[[\"label\", \"title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Subset-exploded stage counts per evidence\n",
    "\n",
    "Stacked bar charts showing the distribution of individual stages chosen across all verdicts,\n",
    "plus abstain rates. Each verdict is exploded into its component stages (e.g., `[2,3]` → stage 2 + stage 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Subset-exploded stage counts per evidence ---\n",
    "# Vertical stacked bar charts, one panel per model (2 columns).\n",
    "# Each verdict is exploded into component stages, plus abstain counts.\n",
    "\n",
    "def _explode_stages(row):\n",
    "    \"\"\"Explode a verdict into individual stage selections.\"\"\"\n",
    "    if row[\"abstained\"]:\n",
    "        return [\"ABSTAIN\"]\n",
    "    else:\n",
    "        return row[\"decodedScores\"]\n",
    "\n",
    "# Build exploded stage counts per evidence per model\n",
    "n_models = len(data.tags)\n",
    "n_cols = 2\n",
    "n_rows = (n_models + n_cols - 1) // n_cols  # Ceiling division\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10 * n_cols, 6 * n_rows), sharey=True)\n",
    "\n",
    "# Flatten axes array for easier iteration\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "# Get all stages + ABSTAIN\n",
    "all_stages = list(range(1, data.scale_size + 1)) + [\"ABSTAIN\"]\n",
    "\n",
    "# Heat scale colors: stage 1 (coolest) to stage 4 (hottest), then grey for ABSTAIN\n",
    "# Using YlOrRd colormap for heat scale\n",
    "heat_colors = plt.cm.YlOrRd(np.linspace(0.3, 0.9, data.scale_size))\n",
    "stage_colors = list(heat_colors) + [(0.7, 0.7, 0.7)]  # Add grey for ABSTAIN\n",
    "\n",
    "for idx, tag in enumerate(data.tags):\n",
    "    ax = axes[idx]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = scores[scores[\"experimentTag\"] == tag].copy()\n",
    "    \n",
    "    # Explode verdicts into individual stages\n",
    "    sub[\"stages\"] = sub.apply(_explode_stages, axis=1)\n",
    "    exploded = sub.explode(\"stages\")\n",
    "    \n",
    "    # Count stages per evidence\n",
    "    ev_labels = sorted(eid_to_label.values(), key=lambda l: int(l[1:]))\n",
    "    stage_counts = (\n",
    "        exploded.groupby([\"evidence\", \"stages\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    \n",
    "    # Reindex to ensure all stages and evidence are present\n",
    "    stage_counts = stage_counts.reindex(index=ev_labels, columns=all_stages, fill_value=0)\n",
    "    \n",
    "    # Create vertical stacked bar chart\n",
    "    stage_counts.plot(\n",
    "        kind=\"bar\",\n",
    "        stacked=True,\n",
    "        ax=ax,\n",
    "        color=stage_colors,\n",
    "        width=0.7,\n",
    "        legend=(idx == n_models - 1),  # Only show legend on last panel\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"{model}\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_xlabel(\"Evidence\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    ax.set_xticklabels(ev_labels, rotation=0)\n",
    "    \n",
    "    # Add count annotations on bars\n",
    "    for i, evidence in enumerate(ev_labels):\n",
    "        cumulative = 0\n",
    "        for stage in all_stages:\n",
    "            count = stage_counts.loc[evidence, stage]\n",
    "            if count > 0:\n",
    "                ax.text(\n",
    "                    i,\n",
    "                    cumulative + count / 2,\n",
    "                    str(int(count)),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8,\n",
    "                    fontweight=\"bold\",\n",
    "                    color=\"white\" if count > 5 else \"black\",\n",
    "                )\n",
    "                cumulative += count\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "# Adjust legend\n",
    "if n_models > 0:\n",
    "    axes[n_models - 1].legend(\n",
    "        title=\"Stage\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\",\n",
    "        frameon=True,\n",
    "    )\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Subset-exploded stage counts per evidence\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=0.98,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Rubric stage length bias analysis\n",
    "\n",
    "Following Dubois et al. (2024), we test whether models exhibit systematic bias toward longer\n",
    "or shorter rubric stage descriptions. We run both pooled (across all rubrics) and per-rubric\n",
    "regressions to assess length effects.\n",
    "\n",
    "**Key finding**: Length bias is **highly heterogeneous across rubrics** — some rubrics show\n",
    "strong positive bias (prefer longer stages), others show strong negative bias (prefer shorter),\n",
    "with effects ranging from -0.29 to +0.30. The pooled effect averages to near-zero, masking\n",
    "this variation. Given this heterogeneity, we **do not apply a length discount** — any global\n",
    "correction would be arbitrary and could amplify bias in rubrics where the effect runs opposite\n",
    "to the average.\n",
    "\n",
    "### Notes\n",
    "- Given Multiple testing: With 30 rubrics × 3 models = 90 tests, ~3 will be \"significant\" by chance at p < 0.05. Could use Bonferroni correction (p < 0.05/60 = 0.0008) Or FDR correction (less conservative).\n",
    "- Could use a signficance based adjustment when there's statistical evidence of length bias with per rubric, adjusted by direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rubric stage length bias regression ---\n",
    "\n",
    "# Flatten rubric quality stats if present\n",
    "rubrics = data.rubrics.copy()\n",
    "if rubrics.empty:\n",
    "    raise ValueError(\"No rubrics returned. Check Convex endpoint.\")\n",
    "\n",
    "if \"qualityStats\" in rubrics.columns:\n",
    "    rubrics[\"observabilityScore\"] = rubrics[\"qualityStats\"].apply(\n",
    "        lambda q: q.get(\"observabilityScore\") if isinstance(q, dict) else np.nan\n",
    "    )\n",
    "    rubrics[\"discriminabilityScore\"] = rubrics[\"qualityStats\"].apply(\n",
    "        lambda q: q.get(\"discriminabilityScore\") if isinstance(q, dict) else np.nan\n",
    "    )\n",
    "\n",
    "def _word_count(text: str) -> int:\n",
    "    \"\"\"Count words in a text string.\"\"\"\n",
    "    tokens = re.split(r\"\\s+\", (text or \"\").strip())\n",
    "    return len([t for t in tokens if t])\n",
    "\n",
    "def _stage_text(stage: dict) -> str:\n",
    "    \"\"\"Concatenate stage label + criteria into a single text string.\"\"\"\n",
    "    parts = [stage.get(\"label\", \"\")]\n",
    "    parts.extend(stage.get(\"criteria\", []))\n",
    "    return \" \".join([p for p in parts if p])\n",
    "\n",
    "# Build stage-level table: one row per rubric stage\n",
    "stage_rows = []\n",
    "for _, r in rubrics.iterrows():\n",
    "    stages = r.get(\"stages\") or []\n",
    "    for idx, stage in enumerate(stages, start=1):\n",
    "        stage_rows.append({\n",
    "            \"rubricId\": r[\"rubricId\"],\n",
    "            \"stage\": idx,\n",
    "            \"stage_len\": _word_count(_stage_text(stage)),\n",
    "            \"observabilityScore\": r.get(\"observabilityScore\"),\n",
    "            \"discriminabilityScore\": r.get(\"discriminabilityScore\"),\n",
    "        })\n",
    "\n",
    "stage_df = pd.DataFrame(stage_rows)\n",
    "\n",
    "# Z-score stage length within each rubric\n",
    "stage_df[\"stage_len_z\"] = stage_df.groupby(\"rubricId\")[\"stage_len\"].transform(\n",
    "    lambda s: (s - s.mean()) / s.std(ddof=0) if s.std(ddof=0) > 0 else 0.0\n",
    ")\n",
    "\n",
    "# Add score ID before expanding (so we can group back later)\n",
    "scores[\"scoreId\"] = scores.index\n",
    "\n",
    "# Expand scores to score × stage rows for regression\n",
    "score_stage = scores.merge(stage_df, on=\"rubricId\", how=\"left\")\n",
    "score_stage[\"selected\"] = score_stage.apply(\n",
    "    lambda r: 0 if r[\"abstained\"] or r[\"decodedScores\"] is None\n",
    "    else int(r[\"stage\"] in r[\"decodedScores\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Fit stage-length bias per model (linear probability model)\n",
    "# DV: selected (0/1), IV: stage_len_z + fixed effects for evidence + rubric quality\n",
    "betas = {}\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = score_stage[score_stage[\"experimentTag\"] == tag].copy()\n",
    "    sub = sub[~sub[\"abstained\"]]  # exclude abstains from regression\n",
    "    if sub.empty:\n",
    "        betas[model] = 0.0\n",
    "        continue\n",
    "    formula = \"selected ~ stage_len_z + C(evidence) + observabilityScore + discriminabilityScore\"\n",
    "    res = smf.ols(formula, data=sub).fit()\n",
    "    betas[model] = float(res.params.get(\"stage_len_z\", 0.0))\n",
    "    print(f\"{model}: stage_len_z beta = {betas[model]:.4f} (n={len(sub)}, p={res.pvalues.get('stage_len_z', np.nan):.4f})\")\n",
    "\n",
    "# Rubric quality proxy and final adjusted probe\n",
    "rubric_quality = stage_df.groupby(\"rubricId\")[[\"observabilityScore\", \"discriminabilityScore\"]].first().reset_index()\n",
    "scores = scores.merge(rubric_quality, on=\"rubricId\", how=\"left\")\n",
    "scores[\"p_rubric\"] = scores[\"observabilityScore\"] * scores[\"discriminabilityScore\"]\n",
    "scores[\"p_score\"] = scores[\"expertAgreementProb\"]\n",
    "scores[\"p_adjusted\"] = scores[\"p_score\"] * scores[\"p_rubric\"]  # No length discount\n",
    "\n",
    "# Per-rubric regressions to see heterogeneity in length bias\n",
    "print(\"\\n=== Per-rubric length bias regressions ===\")\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = score_stage[score_stage[\"experimentTag\"] == tag].copy()\n",
    "    sub = sub[~sub[\"abstained\"]]\n",
    "    \n",
    "    rubric_results = []\n",
    "    for rubric_id in sub[\"rubricId\"].unique():\n",
    "        rubric_sub = sub[sub[\"rubricId\"] == rubric_id].copy()\n",
    "        \n",
    "        # Need at least some variation to fit\n",
    "        if rubric_sub[\"selected\"].nunique() < 2 or len(rubric_sub) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Simple regression: selected ~ stage_len_z (no other controls, just length effect)\n",
    "        try:\n",
    "            formula = \"selected ~ stage_len_z\"\n",
    "            res_rubric = smf.ols(formula, data=rubric_sub).fit()\n",
    "            \n",
    "            rubric_results.append({\n",
    "                \"rubricId\": rubric_id,\n",
    "                \"beta\": res_rubric.params.get(\"stage_len_z\", np.nan),\n",
    "                \"pvalue\": res_rubric.pvalues.get(\"stage_len_z\", np.nan),\n",
    "                \"n_obs\": len(rubric_sub),\n",
    "                \"n_selected\": rubric_sub[\"selected\"].sum(),\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rubric_df = pd.DataFrame(rubric_results)\n",
    "    rubric_df[\"selection_rate\"] = rubric_df[\"n_selected\"] / rubric_df[\"n_obs\"]\n",
    "    \n",
    "    \n",
    "    print(f\"\\n--- {model} ---\")\n",
    "    print(f\"Overall beta: {betas[model]:.4f}\")\n",
    "    print(f\"Rubrics analyzed: {len(rubric_df)}\")\n",
    "    print(f\"\\nTop 10 rubrics by absolute beta (strongest length effects):\")\n",
    "    \n",
    "    top_rubrics = rubric_df.sort_values(\"beta\", key=abs, ascending=False).head(10)\n",
    "    display(top_rubrics[[\"rubricId\", \"beta\", \"pvalue\", \"selection_rate\", \"n_obs\"]].style.format({\n",
    "        \"beta\": \"{:.4f}\",\n",
    "        \"pvalue\": \"{:.4f}\",\n",
    "        \"selection_rate\": \"{:.3f}\",\n",
    "        \"n_obs\": \"{:.0f}\",\n",
    "    }).hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Abstain & specificity rates per evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstain and singleton rates (uses scores + eid_to_label from regression cell above)\n",
    "\n",
    "def rates_table(scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Per model: abstain % and singleton-commit % for each evidence.\n",
    "\n",
    "    Columns use 'E1 (N=30)' notation so the sample size is in the header.\n",
    "    Rows are multi-indexed: (model, metric).\n",
    "    \"\"\"\n",
    "    labels = sorted(eid_to_label.values(), key=lambda l: int(l[1:]))\n",
    "\n",
    "    records = []\n",
    "    for tag in data.tags:\n",
    "        sub = scores[scores[\"experimentTag\"] == tag]\n",
    "        model = data.experiments[tag][\"modelId\"]\n",
    "        abstain_row = {}\n",
    "        single_row = {}\n",
    "\n",
    "        for label in labels:\n",
    "            g = sub[sub[\"evidence\"] == label]\n",
    "            n = len(g)\n",
    "            col = f\"{label} (N={n})\"\n",
    "            # Abstain rate\n",
    "            a = g[\"abstained\"].sum()\n",
    "            abstain_row[col] = f\"{a / n * 100:.0f}%\" if n else \"—\"\n",
    "            # Singleton rate (of non-abstained)\n",
    "            non_abs = g[~g[\"abstained\"]]\n",
    "            nn = len(non_abs)\n",
    "            s = non_abs[\"decodedScores\"].apply(len).eq(1).sum() if nn else 0\n",
    "            single_row[col] = f\"{s / nn * 100:.0f}%\" if nn else \"—\"\n",
    "\n",
    "        # Total column\n",
    "        n_total = len(sub)\n",
    "        a_total = sub[\"abstained\"].sum()\n",
    "        non_abs_total = sub[~sub[\"abstained\"]]\n",
    "        nn_total = len(non_abs_total)\n",
    "        s_total = non_abs_total[\"decodedScores\"].apply(len).eq(1).sum() if nn_total else 0\n",
    "\n",
    "        abstain_row[f\"Total (N={n_total})\"] = f\"{a_total / n_total * 100:.0f}%\"\n",
    "        single_row[f\"Total (N={n_total})\"] = f\"{s_total / nn_total * 100:.0f}%\" if nn_total else \"—\"\n",
    "\n",
    "        records.append((model, \"Abstain %\", abstain_row))\n",
    "        records.append((model, \"Singleton %\", single_row))\n",
    "\n",
    "    idx = pd.MultiIndex.from_tuples([(r[0], r[1]) for r in records])\n",
    "    return pd.DataFrame([r[2] for r in records], index=idx)\n",
    "\n",
    "rates_table(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Verdict distribution heatmaps with proportions and average expert probe\n",
    "\n",
    "Each cell displays **(proportion, avg expertAgreementProb)** for that evidence-verdict combination,\n",
    "both formatted with 2 decimal places. All responses — **including abstains** — are included. \n",
    "Each row (evidence) is normalized to sum to 1.\n",
    "\n",
    "One panel per model, stacked vertically. Columns are powerset verdicts sorted by\n",
    "center-of-gravity, then cardinality. Empty cells (0.00) are left blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verdict distribution heatmaps with proportions and average expert probe ---\n",
    "# Two panels per row (2 columns). Evidence on y-axis, powerset verdicts on x-axis.\n",
    "# Each cell shows (proportion [0,1], avg_expertAgreementProb) with 2 sig figs.\n",
    "# Rows are normalized to sum to 1. A thin visual separator is drawn before ABSTAIN.\n",
    "\n",
    "def _col_sort_key(col: str):\n",
    "    \"\"\"Sort verdict columns: by center-of-gravity, then cardinality, then elements.\n",
    "    ABSTAIN always sorts last.\"\"\"\n",
    "    if col == \"ABSTAIN\":\n",
    "        return (999, 0, [])\n",
    "    stages = ast.literal_eval(col)  # e.g. \"[2, 3]\" -> [2, 3]\n",
    "    cog = sum(stages) / len(stages)\n",
    "    return (cog, len(stages), stages)\n",
    "\n",
    "# Collect all verdicts across all models\n",
    "tmp_all = scores.copy()\n",
    "tmp_all[\"verdict\"] = tmp_all.apply(\n",
    "    lambda r: \"ABSTAIN\" if r[\"abstained\"] else str(sorted(r[\"decodedScores\"])),\n",
    "    axis=1,\n",
    ")\n",
    "all_verdicts = sorted(tmp_all[\"verdict\"].unique(), key=_col_sort_key)\n",
    "\n",
    "def _build_count_and_avg_matrices(sub: pd.DataFrame, all_verdicts: list) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Build evidence × verdict count matrix and average expertAgreementProb matrix.\"\"\"\n",
    "    ev_labels = sorted(eid_to_label.values(), key=lambda l: int(l[1:]))\n",
    "    tmp = sub.copy()\n",
    "    tmp[\"verdict\"] = tmp.apply(\n",
    "        lambda r: \"ABSTAIN\" if r[\"abstained\"] else str(sorted(r[\"decodedScores\"])),\n",
    "        axis=1,\n",
    "    )\n",
    "    tmp[\"expertAgreementProb\"] = tmp[\"expertAgreementProb\"].fillna(1.0)\n",
    "    \n",
    "    # Count matrix\n",
    "    count_pivot = (\n",
    "        tmp.groupby([\"evidence\", \"verdict\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=ev_labels, columns=all_verdicts, fill_value=0)\n",
    "    )\n",
    "    \n",
    "    # Average expertAgreementProb matrix\n",
    "    avg_pivot = (\n",
    "        tmp.groupby([\"evidence\", \"verdict\"])[\"expertAgreementProb\"]\n",
    "        .mean()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=ev_labels, columns=all_verdicts, fill_value=0)\n",
    "    )\n",
    "    \n",
    "    return count_pivot, avg_pivot\n",
    "\n",
    "def _insert_thin_separator(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Insert a narrow NaN spacer column before ABSTAIN for a small visual gap.\"\"\"\n",
    "    if \"ABSTAIN\" not in df.columns:\n",
    "        return df\n",
    "    cols = [c for c in df.columns if c != \"ABSTAIN\"]\n",
    "    sep = pd.DataFrame(np.nan, index=df.index, columns=[\" \"])\n",
    "    return pd.concat([df[cols], sep, df[[\"ABSTAIN\"]]], axis=1)\n",
    "\n",
    "n_models = len(data.tags)\n",
    "n_rows = (n_models + 1) // 2  # 2 charts per row\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(34, 6 * n_rows))\n",
    "if n_models == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, tag in enumerate(data.tags):\n",
    "    ax = axes_flat[idx]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = scores[scores[\"experimentTag\"] == tag]\n",
    "\n",
    "    count_mat, avg_mat = _build_count_and_avg_matrices(sub, all_verdicts)\n",
    "\n",
    "    # Convert counts to proportions (row-normalized to [0, 1])\n",
    "    prop_mat = count_mat.div(count_mat.sum(axis=1), axis=0)\n",
    "\n",
    "    # Insert thin spacer before ABSTAIN\n",
    "    prop_mat = _insert_thin_separator(prop_mat)\n",
    "    avg_mat = _insert_thin_separator(avg_mat)\n",
    "\n",
    "    mask = prop_mat.isna()\n",
    "    plot_data = prop_mat.fillna(0)\n",
    "\n",
    "    # Use proportion for heatmap color intensity\n",
    "    sns.heatmap(\n",
    "        plot_data,\n",
    "        annot=False,\n",
    "        mask=mask,\n",
    "        cmap=\"YlOrRd\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        linewidths=0.5,\n",
    "        cbar=True,\n",
    "        cbar_kws={\"label\": \"Proportion\"},\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Annotations: (proportion, avg_prob) with 2 sig figs\n",
    "    for row_i in range(plot_data.shape[0]):\n",
    "        for col_j in range(plot_data.shape[1]):\n",
    "            prop = prop_mat.iloc[row_i, col_j]\n",
    "            avg_prob = avg_mat.iloc[row_i, col_j]\n",
    "            if pd.isna(prop) or prop == 0:\n",
    "                continue\n",
    "            \n",
    "            # Format text with 2 sig figs\n",
    "            text = f\"({prop:.2f}, {avg_prob:.2f})\"\n",
    "            \n",
    "            # Alpha based on proportion\n",
    "            norm = mplNormalize(vmin=0, vmax=1)\n",
    "            alpha = max(0.3, norm(prop))\n",
    "            \n",
    "            ax.text(\n",
    "                col_j + 0.5, row_i + 0.5,\n",
    "                text,\n",
    "                ha=\"center\", va=\"center\",\n",
    "                fontsize=8, fontweight=\"bold\",\n",
    "                color=(0, 0, 0, alpha),\n",
    "            )\n",
    "\n",
    "    # Thin white stripe over the spacer column\n",
    "    spacer_idx = None\n",
    "    for ci, c in enumerate(prop_mat.columns):\n",
    "        if c == \" \":\n",
    "            spacer_idx = ci\n",
    "            break\n",
    "    if spacer_idx is not None:\n",
    "        ax.axvline(x=spacer_idx, color=\"white\", linewidth=3, zorder=5)\n",
    "        ax.axvline(x=spacer_idx + 1, color=\"white\", linewidth=3, zorder=5)\n",
    "\n",
    "    ax.set_title(f\"{model}\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Evidence\")\n",
    "    ax.set_xlabel(\"Verdict\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(n_models, len(axes_flat)):\n",
    "    axes_flat[idx].set_visible(False)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Verdict distribution per evidence: (proportion, avg expertAgreementProb)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=0.995,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Belief Function Analysis (Transferable Belief Model)\n",
    "\n",
    "We model each LLM response as a **mass function** in the Dempster-Shafer / Smets TBM framework,\n",
    "using an **open-world assumption** where mass on the empty set represents contradiction.\n",
    "\n",
    "**Frame of discernment:** `Theta = {1, 2, ..., scale_size}` (the ordinal rubric stages).\n",
    "\n",
    "**Mass assignment rules** (let `p = expertAgreementProb`):\n",
    "\n",
    "| Response type | m(verdict) | m(Theta) | m({}) |\n",
    "|---|---|---|---|\n",
    "| **Normal verdict** (proper subset, e.g. `{2,3}`) | p | 1 - p | 0 |\n",
    "| **Full frame** (model chose all stages) | -- | p | 1 - p |\n",
    "| **Abstain** (model refused) | -- | 1 - p | p |\n",
    "\n",
    "- **Normal verdict**: standard simple support function. The probe partitions between the specific verdict and ignorance.\n",
    "- **Full frame**: a confident \"could be anything\" is genuine ignorance; an unconfident one is treated as contradiction.\n",
    "- **Abstain**: a confident refusal is genuine contradiction; an unconfident one is closer to ignorance.\n",
    "\n",
    "Full-frame and abstain are **symmetric mirrors** on the ignorance-contradiction axis, with the probe as the pivot.\n",
    "\n",
    "We compute mass functions **per rubric** (each rubric is a stochastic draw from the design space),\n",
    "then extract pignistic probabilities to see what each rubric \"thinks\" about each evidence article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build per-rubric TBM mass functions ---\n",
    "\n",
    "theta = frozenset(range(1, data.scale_size + 1))  # e.g. frozenset({1, 2, 3, 4})\n",
    "\n",
    "def response_to_mass(row: pd.Series, theta: frozenset) -> MassFunction:\n",
    "    \"\"\"\n",
    "    Convert a single model response to a Dempster-Shafer mass function (TBM).\n",
    "\n",
    "    Rules (let p = expertAgreementProb):\n",
    "      - Normal verdict (proper subset of Theta): m(verdict) = p, m(Theta) = 1-p\n",
    "      - Full frame (verdict == Theta):           m(Theta) = p, m({}) = 1-p\n",
    "      - Abstain:                                 m({}) = p,    m(Theta) = 1-p\n",
    "    \"\"\"\n",
    "    p = float(row.get(\"expertAgreementProb\") or 1.0)\n",
    "\n",
    "    if row[\"abstained\"]:\n",
    "        # Abstain: probe -> contradiction, remainder -> ignorance\n",
    "        m = MassFunction()\n",
    "        m[frozenset()] = p        # contradiction\n",
    "        m[theta] = 1.0 - p        # ignorance\n",
    "        return m\n",
    "\n",
    "    verdict = frozenset(int(s) for s in row[\"decodedScores\"])\n",
    "\n",
    "    if verdict == theta:\n",
    "        # Full frame: probe -> ignorance, remainder -> contradiction\n",
    "        m = MassFunction()\n",
    "        m[theta] = p               # genuine ignorance\n",
    "        m[frozenset()] = 1.0 - p   # contradiction\n",
    "        return m\n",
    "\n",
    "    # Normal verdict: simple support function\n",
    "    m = MassFunction()\n",
    "    m[verdict] = p                 # specific verdict\n",
    "    m[theta] = 1.0 - p            # ignorance\n",
    "    return m\n",
    "\n",
    "\n",
    "# Build per-rubric mass functions and extract pignistic probabilities\n",
    "stages = sorted(theta)  # [1, 2, 3, 4]\n",
    "\n",
    "per_rubric_rows = []\n",
    "for _, row in scores.iterrows():\n",
    "    tag = row[\"experimentTag\"]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    ev_label = row[\"evidence\"]\n",
    "    rubric_id = row[\"rubricId\"]\n",
    "\n",
    "    # Build the individual mass function\n",
    "    m = response_to_mass(row, theta)\n",
    "\n",
    "    # Pignistic transformation\n",
    "    pign = m.pignistic()\n",
    "\n",
    "    rec = {\n",
    "        \"model\": model,\n",
    "        \"tag\": tag,\n",
    "        \"evidence\": ev_label,\n",
    "        \"rubricId\": rubric_id,\n",
    "        \"p_score\": row.get(\"expertAgreementProb\", np.nan),\n",
    "        \"abstained\": row[\"abstained\"],\n",
    "        \"verdict\": \"ABSTAIN\" if row[\"abstained\"] else str(sorted(row[\"decodedScores\"])),\n",
    "        \"conflict\": m[frozenset()],\n",
    "    }\n",
    "    for s in stages:\n",
    "        singleton = frozenset({s})\n",
    "        rec[f\"betP_{s}\"] = pign[singleton] if singleton in pign else 0.0\n",
    "\n",
    "    # Max pignistic probability (conviction measure)\n",
    "    rec[\"max_betP\"] = max([rec[f\"betP_{s}\"] for s in stages])\n",
    "    rec[\"modal_stage\"] = max(stages, key=lambda s: rec[f\"betP_{s}\"])\n",
    "\n",
    "    per_rubric_rows.append(rec)\n",
    "\n",
    "    per_rubric_rows.append(rec)\n",
    "\n",
    "per_rubric_df = pd.DataFrame(per_rubric_rows)\n",
    "\n",
    "print(f\"Built {len(per_rubric_df)} per-rubric mass functions\")\n",
    "print(f\"Frame: Theta = {set(theta)}\")\n",
    "\n",
    "\n",
    "# --- Combine mass functions per rubric (aggregate across all evidence) ---\n",
    "\n",
    "combined_results = []\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = scores[scores[\"experimentTag\"] == tag]\n",
    "\n",
    "    # Get all unique rubrics for this experiment\n",
    "    for rubric_id in sub[\"rubricId\"].unique():\n",
    "        rubric_sub = sub[sub[\"rubricId\"] == rubric_id]\n",
    "        if rubric_sub.empty:\n",
    "            continue\n",
    "\n",
    "        # Build mass functions for all evidence scored by this rubric\n",
    "        masses = [response_to_mass(row, theta) for _, row in rubric_sub.iterrows()]\n",
    "\n",
    "        # Combine via unnormalized conjunctive rule (Smets' TBM)\n",
    "        combined = reduce(\n",
    "            lambda a, b: a.combine_conjunctive(b, normalization=False),\n",
    "            masses,\n",
    "        )\n",
    "\n",
    "        # Extract DST metrics\n",
    "        conflict = combined[frozenset()]\n",
    "        \n",
    "        # Handle edge case: if conflict = 1.0, pignistic is undefined\n",
    "        if conflict >= 0.9999:\n",
    "            pign = {}  # No belief to distribute\n",
    "        else:\n",
    "            pign = combined.pignistic()\n",
    "\n",
    "        result = {\n",
    "            \"model\": model,\n",
    "            \"tag\": tag,\n",
    "            \"rubricId\": rubric_id,\n",
    "            \"n_evidence\": len(rubric_sub),\n",
    "            \"conflict\": conflict,\n",
    "        }\n",
    "\n",
    "        for s in stages:\n",
    "            singleton = frozenset({s})\n",
    "            result[f\"bel_{s}\"] = combined.bel(singleton)\n",
    "            result[f\"pl_{s}\"] = combined.pl(singleton)\n",
    "            result[f\"betP_{s}\"] = pign[singleton] if singleton in pign else 0.0\n",
    "\n",
    "        # Max pignistic (conviction)\n",
    "        result[\"max_betP\"] = max([result[f\"betP_{s}\"] for s in stages])\n",
    "        result[\"modal_stage\"] = max(stages, key=lambda s: result[f\"betP_{s}\"])\n",
    "\n",
    "        combined_results.append(result)\n",
    "\n",
    "combined_df = pd.DataFrame(combined_results)\n",
    "\n",
    "print(f\"\\nBuilt {len(combined_df)} combined mass functions (per model × rubric)\")\n",
    "\n",
    "# Filter by conflict threshold\n",
    "CONFLICT_THRESHOLD = 0.9\n",
    "\n",
    "print(f\"\\n=== Conflict filtering (threshold = {CONFLICT_THRESHOLD}) ===\")\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = combined_df[combined_df[\"tag\"] == tag].copy()\n",
    "    n_total = len(sub)\n",
    "    n_usable = (sub[\"conflict\"] < CONFLICT_THRESHOLD).sum()\n",
    "    n_unusable = n_total - n_usable\n",
    "    pct_unusable = (n_unusable / n_total * 100) if n_total > 0 else 0\n",
    "    print(f\"{model}: {n_unusable}/{n_total} unusable ({pct_unusable:.1f}% with conflict ≥ {CONFLICT_THRESHOLD})\")\n",
    "\n",
    "# Display top 10 most convictional rubrics per experiment (filtered)\n",
    "print(f\"\\n=== Top 10 most convictional rubrics per experiment (conflict < {CONFLICT_THRESHOLD}) ===\")\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = combined_df[combined_df[\"tag\"] == tag].copy()\n",
    "    \n",
    "    # Filter by conflict threshold\n",
    "    sub_filtered = sub[sub[\"conflict\"] < CONFLICT_THRESHOLD].copy()\n",
    "    \n",
    "    if sub_filtered.empty:\n",
    "        print(f\"\\n--- {model} ---\")\n",
    "        print(f\"No rubrics with conflict < {CONFLICT_THRESHOLD}\")\n",
    "        continue\n",
    "    \n",
    "    top10 = sub_filtered.nlargest(10, \"max_betP\")\n",
    "    \n",
    "    print(f\"\\n--- {model} ({len(sub_filtered)} usable, showing top {len(top10)}) ---\")\n",
    "    \n",
    "    # Build display table with [Bel,Pl] intervals\n",
    "    disp_rows = []\n",
    "    for _, r in top10.iterrows():\n",
    "        row_data = {\n",
    "            \"rubricId\": r[\"rubricId\"],\n",
    "            \"conflict\": r[\"conflict\"],\n",
    "        }\n",
    "        for s in stages:\n",
    "            row_data[f\"[Bel,Pl]({s})\"] = f\"[{r[f'bel_{s}']:.3f}, {r[f'pl_{s}']:.3f}]\"\n",
    "            row_data[f\"BetP({s})\"] = r[f\"betP_{s}\"]\n",
    "        disp_rows.append(row_data)\n",
    "    \n",
    "    disp_df = pd.DataFrame(disp_rows)\n",
    "    \n",
    "    # Apply styling: bold the winning BetP per row\n",
    "    def highlight_max_betP(row):\n",
    "        betP_cols = [f\"BetP({s})\" for s in stages]\n",
    "        betP_vals = [row[col] for col in betP_cols]\n",
    "        max_val = max(betP_vals)\n",
    "        return [\n",
    "            \"font-weight: bold\" if col in betP_cols and row[col] == max_val else \"\"\n",
    "            for col in row.index\n",
    "        ]\n",
    "    \n",
    "    display(disp_df.style\n",
    "        .format({\n",
    "            \"conflict\": \"{:.3f}\",\n",
    "            **{f\"BetP({s})\": \"{:.3f}\" for s in stages},\n",
    "        })\n",
    "        .apply(highlight_max_betP, axis=1)\n",
    "        .hide(axis=\"index\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Closed-World DST Analysis\n",
    "\n",
    "Alternative analysis using **classical Dempster-Shafer** (closed-world assumption):\n",
    "\n",
    "**Key differences from TBM:**\n",
    "- **Drop abstentions** entirely (no mass on empty set)\n",
    "- **Full-frame responses** treated as pure ignorance: `m(Theta) = 1.0`\n",
    "- **Normalized combination** (Dempster's rule with conflict redistribution)\n",
    "\n",
    "**Mass assignment rules** (let `p = expertAgreementProb`):\n",
    "\n",
    "| Response type | m(verdict) | m(Theta) |\n",
    "|---|---|---|\n",
    "| **Normal verdict** (proper subset) | p | 1 - p |\n",
    "| **Full frame** (all stages) | -- | 1.0 |\n",
    "| **Abstain** | *dropped* | *dropped* |\n",
    "\n",
    "This gives us a \"best-case\" view: what do the rubrics say when we only consider their substantive judgments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Closed-World DST: Build per-rubric mass functions ---\n",
    "\n",
    "def response_to_mass_closed(row: pd.Series, theta: frozenset) -> MassFunction | None:\n",
    "    \"\"\"\n",
    "    Convert a single model response to a classical DST mass function (closed-world).\n",
    "\n",
    "    Rules (let p = expertAgreementProb):\n",
    "      - Normal verdict (proper subset): m(verdict) = p, m(Theta) = 1-p\n",
    "      - Full frame (verdict == Theta):  m(Theta) = 1.0 (pure ignorance)\n",
    "      - Abstain:                        None (dropped)\n",
    "    \"\"\"\n",
    "    # Drop abstentions\n",
    "    if row[\"abstained\"]:\n",
    "        return None\n",
    "\n",
    "    p = float(row.get(\"expertAgreementProb\") or 1.0)\n",
    "    verdict = frozenset(int(s) for s in row[\"decodedScores\"])\n",
    "\n",
    "    # Full frame: pure ignorance\n",
    "    if verdict == theta:\n",
    "        m = MassFunction()\n",
    "        m[theta] = 1.0\n",
    "        return m\n",
    "\n",
    "    # Normal verdict: simple support function\n",
    "    m = MassFunction()\n",
    "    m[verdict] = p\n",
    "    m[theta] = 1.0 - p\n",
    "    return m\n",
    "\n",
    "\n",
    "# Build per-rubric mass functions (closed-world)\n",
    "per_rubric_closed_rows = []\n",
    "\n",
    "for _, row in scores.iterrows():\n",
    "    m = response_to_mass_closed(row, theta)\n",
    "    \n",
    "    # Skip abstentions\n",
    "    if m is None:\n",
    "        continue\n",
    "\n",
    "    tag = row[\"experimentTag\"]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    ev_label = row[\"evidence\"]\n",
    "    rubric_id = row[\"rubricId\"]\n",
    "\n",
    "    # Pignistic transformation\n",
    "    pign = m.pignistic()\n",
    "\n",
    "    rec = {\n",
    "        \"model\": model,\n",
    "        \"tag\": tag,\n",
    "        \"evidence\": ev_label,\n",
    "        \"rubricId\": rubric_id,\n",
    "        \"p_score\": row.get(\"expertAgreementProb\", np.nan),\n",
    "        \"verdict\": str(sorted(row[\"decodedScores\"])),\n",
    "    }\n",
    "    \n",
    "    for s in stages:\n",
    "        singleton = frozenset({s})\n",
    "        rec[f\"betP_{s}\"] = pign[singleton] if singleton in pign else 0.0\n",
    "\n",
    "    rec[\"max_betP\"] = max([rec[f\"betP_{s}\"] for s in stages])\n",
    "    rec[\"modal_stage\"] = max(stages, key=lambda s: rec[f\"betP_{s}\"])\n",
    "\n",
    "    per_rubric_closed_rows.append(rec)\n",
    "\n",
    "per_rubric_closed_df = pd.DataFrame(per_rubric_closed_rows)\n",
    "\n",
    "print(f\"Built {len(per_rubric_closed_df)} per-rubric mass functions (closed-world)\")\n",
    "print(f\"Dropped {len(scores) - len(per_rubric_closed_df)} abstentions\")\n",
    "print(f\"Frame: Theta = {set(theta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Closed-World DST: Combine per rubric with normalized rule ---\n",
    "\n",
    "combined_closed_results = []\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = scores[scores[\"experimentTag\"] == tag]\n",
    "\n",
    "    for rubric_id in sub[\"rubricId\"].unique():\n",
    "        rubric_sub = sub[sub[\"rubricId\"] == rubric_id]\n",
    "        if rubric_sub.empty:\n",
    "            continue\n",
    "\n",
    "        # Build mass functions, filtering out abstentions\n",
    "        masses = []\n",
    "        for _, row in rubric_sub.iterrows():\n",
    "            m = response_to_mass_closed(row, theta)\n",
    "            if m is not None:\n",
    "                masses.append(m)\n",
    "\n",
    "        # Skip if no valid responses\n",
    "        if not masses:\n",
    "            continue\n",
    "\n",
    "        # First combine unnormalized to get conflict\n",
    "        combined_unnorm = reduce(\n",
    "            lambda a, b: a.combine_conjunctive(b, normalization=False),\n",
    "            masses,\n",
    "        )\n",
    "        conflict = combined_unnorm[frozenset()]\n",
    "\n",
    "        # Then combine normalized (classic Dempster)\n",
    "        combined = reduce(\n",
    "            lambda a, b: a.combine_conjunctive(b, normalization=True),\n",
    "            masses,\n",
    "        )\n",
    "\n",
    "        # Extract DST metrics\n",
    "        pign = combined.pignistic()\n",
    "\n",
    "        result = {\n",
    "            \"model\": model,\n",
    "            \"tag\": tag,\n",
    "            \"rubricId\": rubric_id,\n",
    "            \"n_evidence\": len(masses),  # count non-abstentions\n",
    "            \"conflict\": conflict,  # conflict that was normalized away\n",
    "        }\n",
    "\n",
    "        for s in stages:\n",
    "            singleton = frozenset({s})\n",
    "            result[f\"bel_{s}\"] = combined.bel(singleton)\n",
    "            result[f\"pl_{s}\"] = combined.pl(singleton)\n",
    "            result[f\"betP_{s}\"] = pign[singleton] if singleton in pign else 0.0\n",
    "\n",
    "        result[\"max_betP\"] = max([result[f\"betP_{s}\"] for s in stages])\n",
    "        result[\"modal_stage\"] = max(stages, key=lambda s: result[f\"betP_{s}\"])\n",
    "\n",
    "        combined_closed_results.append(result)\n",
    "\n",
    "combined_closed_df = pd.DataFrame(combined_closed_results)\n",
    "\n",
    "print(f\"\\nBuilt {len(combined_closed_df)} combined mass functions (closed-world, per model × rubric)\")\n",
    "\n",
    "# Filter by conflict threshold (same as TBM)\n",
    "CONFLICT_THRESHOLD_CLOSED = 0.9\n",
    "\n",
    "print(f\"\\n=== Conflict filtering (threshold = {CONFLICT_THRESHOLD_CLOSED}) ===\")\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = combined_closed_df[combined_closed_df[\"tag\"] == tag].copy()\n",
    "    n_total = len(sub)\n",
    "    n_usable = (sub[\"conflict\"] < CONFLICT_THRESHOLD_CLOSED).sum()\n",
    "    n_unusable = n_total - n_usable\n",
    "    pct_unusable = (n_unusable / n_total * 100) if n_total > 0 else 0\n",
    "    print(f\"{model}: {n_unusable}/{n_total} unusable ({pct_unusable:.1f}% with conflict ≥ {CONFLICT_THRESHOLD_CLOSED})\")\n",
    "\n",
    "# Display top 10 most convictional rubrics per experiment (filtered)\n",
    "print(f\"\\n=== Top 10 most convictional rubrics per experiment (closed-world, conflict < {CONFLICT_THRESHOLD_CLOSED}) ===\")\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = combined_closed_df[combined_closed_df[\"tag\"] == tag].copy()\n",
    "    \n",
    "    # Filter by conflict threshold\n",
    "    sub_filtered = sub[sub[\"conflict\"] < CONFLICT_THRESHOLD_CLOSED].copy()\n",
    "    \n",
    "    if sub_filtered.empty:\n",
    "        print(f\"\\n--- {model} ---\")\n",
    "        print(f\"No rubrics with conflict < {CONFLICT_THRESHOLD_CLOSED}\")\n",
    "        continue\n",
    "    \n",
    "    top10 = sub_filtered.nlargest(10, \"max_betP\")\n",
    "    \n",
    "    print(f\"\\n--- {model} ({len(sub_filtered)} usable, showing top {len(top10)}) ---\")\n",
    "    \n",
    "    # Build display table with conflict and BetP values\n",
    "    disp_rows = []\n",
    "    for _, r in top10.iterrows():\n",
    "        row_data = {\n",
    "            \"rubricId\": r[\"rubricId\"],\n",
    "            \"n_evidence\": int(r[\"n_evidence\"]),\n",
    "            \"conflict\": r[\"conflict\"],\n",
    "        }\n",
    "        for s in stages:\n",
    "            row_data[f\"BetP({s})\"] = r[f\"betP_{s}\"]\n",
    "        disp_rows.append(row_data)\n",
    "    \n",
    "    disp_df = pd.DataFrame(disp_rows)\n",
    "    \n",
    "    # Apply styling: bold the winning BetP per row\n",
    "    def highlight_max_betP(row):\n",
    "        betP_cols = [f\"BetP({s})\" for s in stages]\n",
    "        betP_vals = [row[col] for col in betP_cols]\n",
    "        max_val = max(betP_vals)\n",
    "        return [\n",
    "            \"font-weight: bold\" if col in betP_cols and row[col] == max_val else \"\"\n",
    "            for col in row.index\n",
    "        ]\n",
    "    \n",
    "    display(disp_df.style\n",
    "        .format({\n",
    "            **{f\"BetP({s})\": \"{:.3f}\" for s in stages},\n",
    "        })\n",
    "        .apply(highlight_max_betP, axis=1)\n",
    "        .hide(axis=\"index\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final DST aggregation (weighted by rubric critic scores) ---\n",
    "\n",
    "CONFLICT_THRESHOLD_FINAL = 0.9\n",
    "stage_cols = [f\"betP_{s}\" for s in stages]\n",
    "\n",
    "# One quality weight per (tag, rubric)\n",
    "rubric_weights = (\n",
    "    scores.groupby([\"experimentTag\", \"rubricId\"], as_index=False)[\"p_rubric\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"experimentTag\": \"tag\"})\n",
    ")\n",
    "rubric_weights[\"p_rubric\"] = rubric_weights[\"p_rubric\"].fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "\n",
    "def weighted_quantile(values: np.ndarray, weights: np.ndarray, q: float) -> float:\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    weights = np.asarray(weights, dtype=float)\n",
    "\n",
    "    mask = np.isfinite(values) & np.isfinite(weights)\n",
    "    values = values[mask]\n",
    "    weights = weights[mask]\n",
    "\n",
    "    if len(values) == 0:\n",
    "        return np.nan\n",
    "    if weights.sum() <= 0:\n",
    "        return float(np.quantile(values, q))\n",
    "\n",
    "    sorter = np.argsort(values)\n",
    "    values = values[sorter]\n",
    "    weights = weights[sorter]\n",
    "\n",
    "    cdf = np.cumsum(weights)\n",
    "    cdf = cdf / cdf[-1]\n",
    "    return float(np.interp(q, cdf, values))\n",
    "\n",
    "\n",
    "def aggregate_per_evidence(\n",
    "    per_df: pd.DataFrame,\n",
    "    combined_conflict_df: pd.DataFrame,\n",
    "    method_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    usable = (\n",
    "        combined_conflict_df[combined_conflict_df[\"conflict\"] < CONFLICT_THRESHOLD_FINAL][[\"tag\", \"rubricId\"]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    merged = (\n",
    "        per_df.merge(usable, on=[\"tag\", \"rubricId\"], how=\"inner\")\n",
    "        .merge(rubric_weights, on=[\"tag\", \"rubricId\"], how=\"left\")\n",
    "    )\n",
    "    merged[\"p_rubric\"] = merged[\"p_rubric\"].fillna(0.0)\n",
    "\n",
    "    rows = []\n",
    "    group_cols = [\"model\", \"tag\", \"evidence\"]\n",
    "\n",
    "    for (model, tag, evidence), g in merged.groupby(group_cols):\n",
    "        n_rubrics = int(g[\"rubricId\"].nunique())\n",
    "\n",
    "        for s in stages:\n",
    "            values = g[f\"betP_{s}\"].to_numpy(dtype=float)\n",
    "            weights = g[\"p_rubric\"].to_numpy(dtype=float)\n",
    "\n",
    "            if np.nansum(weights) <= 0:\n",
    "                weights = np.ones_like(values, dtype=float)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"method\": method_name,\n",
    "                    \"model\": model,\n",
    "                    \"tag\": tag,\n",
    "                    \"evidence\": evidence,\n",
    "                    \"stage\": s,\n",
    "                    \"mean_betP\": float(np.average(values, weights=weights)),\n",
    "                    \"q10_betP\": weighted_quantile(values, weights, 0.10),\n",
    "                    \"q90_betP\": weighted_quantile(values, weights, 0.90),\n",
    "                    \"n_rubrics\": n_rubrics,\n",
    "                    \"weight_sum\": float(np.nansum(weights)),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    # Keep evidence in E1..E9 order\n",
    "    out[\"evidence_num\"] = out[\"evidence\"].str.extract(r\"E(\\d+)\").astype(float)\n",
    "    out = out.sort_values([\"model\", \"evidence_num\", \"stage\"]).drop(columns=[\"evidence_num\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "agg_tbm = aggregate_per_evidence(per_rubric_df, combined_df, \"TBM\")\n",
    "agg_closed = aggregate_per_evidence(per_rubric_closed_df, combined_closed_df, \"Closed-world\")\n",
    "agg_all = pd.concat([agg_tbm, agg_closed], ignore_index=True)\n",
    "\n",
    "print(\"Built weighted per-evidence DST aggregates\")\n",
    "print(f\"TBM rows: {len(agg_tbm)} | Closed-world rows: {len(agg_closed)}\")\n",
    "print(f\"Conflict threshold: {CONFLICT_THRESHOLD_FINAL}\")\n",
    "\n",
    "# Quick sanity check: stage probabilities should sum to ~1 per (method, model, evidence)\n",
    "check = (\n",
    "    agg_all.groupby([\"method\", \"model\", \"evidence\"], as_index=False)[\"mean_betP\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"mean_betP\": \"sum_mean_betP\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final visualization: weighted mean BetP heatmaps ---\n",
    "\n",
    "methods = [\"TBM\", \"Closed-world\"]\n",
    "models = [data.experiments[tag][\"modelId\"] for tag in data.tags]\n",
    "\n",
    "n_rows = len(methods)\n",
    "n_cols = len(models)\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows,\n",
    "    n_cols,\n",
    "    figsize=(6 * n_cols, 4.8 * n_rows),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    squeeze=False,\n",
    ")\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    for j, model in enumerate(models):\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        sub = agg_all[(agg_all[\"method\"] == method) & (agg_all[\"model\"] == model)].copy()\n",
    "        pivot = sub.pivot(index=\"evidence\", columns=\"stage\", values=\"mean_betP\")\n",
    "\n",
    "        # Ensure consistent axis order\n",
    "        evidence_order = sorted(pivot.index, key=lambda x: int(re.search(r\"E(\\d+)\", x).group(1)))\n",
    "        pivot = pivot.reindex(index=evidence_order)\n",
    "        pivot = pivot.reindex(columns=stages)\n",
    "\n",
    "        show_cbar = j == (n_cols - 1)\n",
    "        sns.heatmap(\n",
    "            pivot,\n",
    "            ax=ax,\n",
    "            cmap=\"viridis\",\n",
    "            vmin=0.0,\n",
    "            vmax=1.0,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cbar=show_cbar,\n",
    "            cbar_kws={\"label\": \"Weighted mean BetP\"} if show_cbar else None,\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"{method} | {model}\")\n",
    "        ax.set_xlabel(\"Stage\")\n",
    "        ax.set_ylabel(\"Evidence\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Final stage belief per evidence (weighted by rubric critic score, conflict-filtered)\",\n",
    "    y=1.01,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conflict comparison + summary table ---\n",
    "\n",
    "conflict_plot_df = pd.concat(\n",
    "    [\n",
    "        combined_df[[\"model\", \"tag\", \"rubricId\", \"conflict\"]].assign(method=\"TBM\"),\n",
    "        combined_closed_df[[\"model\", \"tag\", \"rubricId\", \"conflict\"]].assign(method=\"Closed-world\"),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "for ax, method in zip(axes, [\"TBM\", \"Closed-world\"]):\n",
    "    sub = conflict_plot_df[conflict_plot_df[\"method\"] == method]\n",
    "    sns.boxplot(data=sub, x=\"model\", y=\"conflict\", ax=ax)\n",
    "    sns.stripplot(data=sub, x=\"model\", y=\"conflict\", ax=ax, color=\"black\", alpha=0.45, size=3)\n",
    "    ax.axhline(CONFLICT_THRESHOLD_FINAL, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_title(f\"{method} conflict distribution\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Conflict\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Summary table (per method x model)\n",
    "summary_rows = []\n",
    "for method, df in [(\"TBM\", combined_df), (\"Closed-world\", combined_closed_df)]:\n",
    "    for tag in data.tags:\n",
    "        model = data.experiments[tag][\"modelId\"]\n",
    "        sub = df[df[\"tag\"] == tag].copy()\n",
    "\n",
    "        n_total = len(sub)\n",
    "        n_usable = int((sub[\"conflict\"] < CONFLICT_THRESHOLD_FINAL).sum())\n",
    "        usable_pct = (100.0 * n_usable / n_total) if n_total else np.nan\n",
    "\n",
    "        usable = sub[sub[\"conflict\"] < CONFLICT_THRESHOLD_FINAL]\n",
    "        mean_conviction = usable[\"max_betP\"].mean() if not usable.empty else np.nan\n",
    "\n",
    "        summary_rows.append(\n",
    "            {\n",
    "                \"method\": method,\n",
    "                \"model\": model,\n",
    "                \"n_total\": n_total,\n",
    "                \"n_usable\": n_usable,\n",
    "                \"usable_pct\": usable_pct,\n",
    "                \"mean_conflict\": sub[\"conflict\"].mean() if n_total else np.nan,\n",
    "                \"median_conflict\": sub[\"conflict\"].median() if n_total else np.nan,\n",
    "                \"mean_max_betP_usable\": mean_conviction,\n",
    "            }\n",
    "        )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values([\"method\", \"model\"])\n",
    "\n",
    "print(f\"Conflict threshold: {CONFLICT_THRESHOLD_FINAL}\")\n",
    "display(\n",
    "    summary_df.style.format(\n",
    "        {\n",
    "            \"usable_pct\": \"{:.1f}%\",\n",
    "            \"mean_conflict\": \"{:.3f}\",\n",
    "            \"median_conflict\": \"{:.3f}\",\n",
    "            \"mean_max_betP_usable\": \"{:.3f}\",\n",
    "        }\n",
    "    ).hide(axis=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Verdict Distribution Ridge Plots & Bootstrap Closed-World DST\n",
    "\n",
    "Two complementary views of verdict behaviour:\n",
    "\n",
    "1. **Discrete ridge plots** — For each (model, evidence), show the raw proportion of each verdict\n",
    "   category (e.g. `[1]`, `[1,2]`, `[2]`, ..., `ABSTAIN`). No smoothing, no invented metric.\n",
    "\n",
    "2. **Bootstrap closed-world DST** — Using classical Dempster's rule (abstentions dropped,\n",
    "   conflict normalized out), resample the per-rubric mass functions (n=1000), re-combine\n",
    "   each time, and report the 95% CI of pignistic probabilities. This answers:\n",
    "   *\"How stable is the DST estimate given only ~30 rubrics?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Discrete ridge plot: verdict proportions per (model, evidence) ---\n",
    "#\n",
    "# For each (model, evidence) cell we compute the proportion of each verdict category\n",
    "# across all rubrics. Verdicts are discrete set-valued categories (e.g. [1], [1,2], [2], ABSTAIN).\n",
    "# No smoothing, no continuous mapping.\n",
    "\n",
    "# Build verdict proportion table from per_rubric_df\n",
    "verdict_props = (\n",
    "    per_rubric_df.groupby([\"model\", \"tag\", \"evidence\", \"verdict\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "verdict_props[\"total\"] = verdict_props.groupby([\"model\", \"tag\", \"evidence\"])[\"count\"].transform(\"sum\")\n",
    "verdict_props[\"proportion\"] = verdict_props[\"count\"] / verdict_props[\"total\"]\n",
    "\n",
    "# Build canonical verdict ordering: singletons first, then pairs, then ABSTAIN\n",
    "all_verdicts_seen = sorted(\n",
    "    verdict_props[\"verdict\"].unique(),\n",
    "    key=lambda v: (\n",
    "        v == \"ABSTAIN\",              # ABSTAIN last\n",
    "        len(ast.literal_eval(v)) if v != \"ABSTAIN\" else 99,  # singletons before pairs\n",
    "        ast.literal_eval(v) if v != \"ABSTAIN\" else [99],     # numeric order within group\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Evidence ordering (E1..E9)\n",
    "evidence_order = sorted(\n",
    "    per_rubric_df[\"evidence\"].unique(),\n",
    "    key=lambda e: int(re.search(r\"\\d+\", e).group()),\n",
    ")\n",
    "\n",
    "print(f\"Verdict categories: {all_verdicts_seen}\")\n",
    "print(f\"Evidence items: {evidence_order}\")\n",
    "print(f\"Models: {verdict_props['model'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ridge plot: one row per evidence, bars = verdict proportions, faceted by model ---\n",
    "\n",
    "n_verdicts = len(all_verdicts_seen)\n",
    "n_evidence = len(evidence_order)\n",
    "n_models = len(data.tags)\n",
    "bar_width = 0.7\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, n_models,\n",
    "    figsize=(5 * n_models, 0.6 * n_evidence * n_verdicts * 0.3 + 2),\n",
    "    sharey=True, sharex=True,\n",
    ")\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Color map for verdict types\n",
    "cmap = plt.cm.Set2\n",
    "verdict_colors = {v: cmap(i / max(n_verdicts - 1, 1)) for i, v in enumerate(all_verdicts_seen)}\n",
    "\n",
    "for ax_idx, tag in enumerate(data.tags):\n",
    "    ax = axes[ax_idx]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = verdict_props[verdict_props[\"tag\"] == tag]\n",
    "\n",
    "    for ev_idx, ev in enumerate(evidence_order):\n",
    "        ev_sub = sub[sub[\"evidence\"] == ev]\n",
    "\n",
    "        # Build full proportion vector (0 for missing verdicts)\n",
    "        props = []\n",
    "        for v in all_verdicts_seen:\n",
    "            match = ev_sub[ev_sub[\"verdict\"] == v]\n",
    "            props.append(float(match[\"proportion\"].iloc[0]) if len(match) > 0 else 0.0)\n",
    "\n",
    "        # Draw horizontal bars stacked from left to right\n",
    "        left = 0.0\n",
    "        for v_idx, (v, p) in enumerate(zip(all_verdicts_seen, props)):\n",
    "            if p > 0:\n",
    "                ax.barh(\n",
    "                    n_evidence - 1 - ev_idx,\n",
    "                    p,\n",
    "                    left=left,\n",
    "                    height=bar_width,\n",
    "                    color=verdict_colors[v],\n",
    "                    edgecolor=\"white\",\n",
    "                    linewidth=0.3,\n",
    "                )\n",
    "                # Label if proportion is large enough\n",
    "                if p >= 0.08:\n",
    "                    ax.text(\n",
    "                        left + p / 2,\n",
    "                        n_evidence - 1 - ev_idx,\n",
    "                        f\"{p:.0%}\",\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        fontsize=6, fontweight=\"bold\",\n",
    "                        color=\"black\", alpha=0.8,\n",
    "                    )\n",
    "                left += p\n",
    "\n",
    "    ax.set_yticks(range(n_evidence))\n",
    "    ax.set_yticklabels(list(reversed(evidence_order)), fontsize=9)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_xlabel(\"Proportion\")\n",
    "    ax.set_title(f\"{model}\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.axvline(0.5, color=\"gray\", linewidth=0.5, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "axes[0].set_ylabel(\"Evidence\")\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_patches = [Patch(facecolor=verdict_colors[v], label=v) for v in all_verdicts_seen]\n",
    "fig.legend(\n",
    "    handles=legend_patches,\n",
    "    loc=\"lower center\",\n",
    "    ncol=min(n_verdicts, 8),\n",
    "    fontsize=8,\n",
    "    title=\"Verdict\",\n",
    "    title_fontsize=9,\n",
    "    bbox_to_anchor=(0.5, -0.02),\n",
    ")\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Verdict distribution per evidence (discrete proportions across rubrics)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.06, 1, 0.96])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bootstrap DST (closed-world): resample per-rubric mass functions, re-combine, get CI on BetP ---\n",
    "#\n",
    "# Uses closed-world DST: abstentions are DROPPED, combination is NORMALIZED (Dempster's rule).\n",
    "# This avoids the TBM problem where abstentions create contradiction that nukes small samples.\n",
    "#\n",
    "# For each (model, evidence), we have ~30 rubric mass functions (minus abstentions).\n",
    "# Bootstrap: resample with replacement (N_BOOT times),\n",
    "# re-combine via normalized Dempster's rule, extract pignistic probabilities.\n",
    "\n",
    "N_BOOT = 1000\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "boot_results = []\n",
    "skipped = []\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    tag_scores = scores[scores[\"experimentTag\"] == tag]\n",
    "\n",
    "    for ev in evidence_order:\n",
    "        ev_scores = tag_scores[tag_scores[\"evidence\"] == ev]\n",
    "        if ev_scores.empty:\n",
    "            continue\n",
    "\n",
    "        # Build closed-world mass functions (drops abstentions)\n",
    "        masses = []\n",
    "        n_abstained = 0\n",
    "        for _, row in ev_scores.iterrows():\n",
    "            m = response_to_mass_closed(row, theta)\n",
    "            if m is not None:\n",
    "                masses.append(m)\n",
    "            else:\n",
    "                n_abstained += 1\n",
    "\n",
    "        n_masses = len(masses)\n",
    "        abstain_rate = n_abstained / (n_masses + n_abstained) if (n_masses + n_abstained) > 0 else 0.0\n",
    "\n",
    "        if n_masses < 2:\n",
    "            skipped.append({\"model\": model, \"evidence\": ev, \"n_active\": n_masses, \"n_abstained\": n_abstained})\n",
    "            continue\n",
    "\n",
    "        # Bootstrap: resample and re-combine with normalized rule\n",
    "        boot_betps = {s: [] for s in stages}\n",
    "        boot_conflicts = []\n",
    "\n",
    "        for _ in range(N_BOOT):\n",
    "            indices = RNG.integers(0, n_masses, size=n_masses)\n",
    "            resampled = [masses[i] for i in indices]\n",
    "\n",
    "            # Unnormalized first (to capture conflict)\n",
    "            combined_unnorm = reduce(\n",
    "                lambda a, b: a.combine_conjunctive(b, normalization=False),\n",
    "                resampled,\n",
    "            )\n",
    "            conflict = combined_unnorm[frozenset()]\n",
    "            boot_conflicts.append(conflict)\n",
    "\n",
    "            # Normalized combination (classical Dempster's rule)\n",
    "            combined = reduce(\n",
    "                lambda a, b: a.combine_conjunctive(b, normalization=True),\n",
    "                resampled,\n",
    "            )\n",
    "\n",
    "            pign = combined.pignistic()\n",
    "            for s in stages:\n",
    "                singleton = frozenset({s})\n",
    "                boot_betps[s].append(pign[singleton] if singleton in pign else 0.0)\n",
    "\n",
    "        # Point estimate (original, no resampling)\n",
    "        orig_combined = reduce(\n",
    "            lambda a, b: a.combine_conjunctive(b, normalization=True),\n",
    "            masses,\n",
    "        )\n",
    "        orig_unnorm = reduce(\n",
    "            lambda a, b: a.combine_conjunctive(b, normalization=False),\n",
    "            masses,\n",
    "        )\n",
    "        orig_conflict = orig_unnorm[frozenset()]\n",
    "        orig_pign = orig_combined.pignistic()\n",
    "\n",
    "        for s in stages:\n",
    "            arr = np.array(boot_betps[s])\n",
    "            singleton = frozenset({s})\n",
    "            boot_results.append({\n",
    "                \"model\": model,\n",
    "                \"tag\": tag,\n",
    "                \"evidence\": ev,\n",
    "                \"stage\": s,\n",
    "                \"betP_point\": orig_pign[singleton] if singleton in orig_pign else 0.0,\n",
    "                \"betP_mean\": float(np.mean(arr)),\n",
    "                \"betP_q025\": float(np.percentile(arr, 2.5)),\n",
    "                \"betP_q975\": float(np.percentile(arr, 97.5)),\n",
    "                \"betP_std\": float(np.std(arr)),\n",
    "                \"conflict_point\": orig_conflict,\n",
    "                \"conflict_mean\": float(np.mean(boot_conflicts)),\n",
    "                \"n_active\": n_masses,\n",
    "                \"n_abstained\": n_abstained,\n",
    "                \"abstain_rate\": abstain_rate,\n",
    "            })\n",
    "\n",
    "boot_df = pd.DataFrame(boot_results)\n",
    "\n",
    "print(f\"Bootstrap closed-world DST: {N_BOOT} iterations\")\n",
    "print(f\"Usable (model, evidence) cells: {len(boot_df) // len(stages)}\")\n",
    "print(f\"Total rows: {len(boot_df)}\")\n",
    "if skipped:\n",
    "    print(f\"\\nSkipped {len(skipped)} cells (< 2 active rubrics after dropping abstentions):\")\n",
    "    for s in skipped:\n",
    "        print(f\"  {s['model']} / {s['evidence']}: {s['n_active']} active, {s['n_abstained']} abstained\")\n",
    "boot_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (placeholder - see next cell for bootstrap visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bootstrap closed-world DST visualization ---\n",
    "#\n",
    "# Bubble chart: one panel per model.\n",
    "# X = stage, Y = evidence.\n",
    "# Circle size = BetP point estimate, color = BetP value.\n",
    "# Vertical error bars = 95% bootstrap CI.\n",
    "# Gray background shading on rows with high abstain rate.\n",
    "\n",
    "n_models = len(data.tags)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    1, n_models,\n",
    "    figsize=(5 * n_models, 0.5 * n_evidence + 2),\n",
    "    sharey=True,\n",
    ")\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax_idx, tag in enumerate(data.tags):\n",
    "    ax = axes[ax_idx]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = boot_df[boot_df[\"tag\"] == tag]\n",
    "\n",
    "    for ev_idx, ev in enumerate(evidence_order):\n",
    "        ev_sub = sub[sub[\"evidence\"] == ev]\n",
    "        y_pos = n_evidence - 1 - ev_idx\n",
    "\n",
    "        if ev_sub.empty:\n",
    "            ax.text(\n",
    "                len(stages) / 2 - 0.5, y_pos,\n",
    "                \"skipped\",\n",
    "                ha=\"center\", va=\"center\", fontsize=7, alpha=0.4, style=\"italic\",\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # Shade row if high abstain rate\n",
    "        abstain_rate = ev_sub[\"abstain_rate\"].iloc[0]\n",
    "        if abstain_rate > 0.3:\n",
    "            ax.axhspan(y_pos - 0.4, y_pos + 0.4, color=\"gray\", alpha=0.08)\n",
    "\n",
    "        for _, row in ev_sub.iterrows():\n",
    "            s = row[\"stage\"]\n",
    "            x_pos = s - 1\n",
    "\n",
    "            point = row[\"betP_point\"]\n",
    "            lo = row[\"betP_q025\"]\n",
    "            hi = row[\"betP_q975\"]\n",
    "\n",
    "            size = max(point * 800, 15)\n",
    "\n",
    "            ax.scatter(\n",
    "                x_pos, y_pos,\n",
    "                s=size,\n",
    "                c=[point],\n",
    "                cmap=\"YlOrRd\",\n",
    "                vmin=0, vmax=1,\n",
    "                edgecolors=\"black\",\n",
    "                linewidths=0.5,\n",
    "                alpha=0.85,\n",
    "                zorder=3,\n",
    "            )\n",
    "\n",
    "            # Vertical error bar for 95% CI\n",
    "            ci_half = (hi - lo) * 2  # Scale for visibility\n",
    "            ax.plot(\n",
    "                [x_pos, x_pos], [y_pos - ci_half, y_pos + ci_half],\n",
    "                color=\"black\", linewidth=1.2, alpha=0.35, zorder=2,\n",
    "            )\n",
    "            cap_w = 0.08\n",
    "            for yy in [y_pos - ci_half, y_pos + ci_half]:\n",
    "                ax.plot(\n",
    "                    [x_pos - cap_w, x_pos + cap_w], [yy, yy],\n",
    "                    color=\"black\", linewidth=1.0, alpha=0.35, zorder=2,\n",
    "                )\n",
    "\n",
    "        # Annotate abstain rate on right edge\n",
    "        if abstain_rate > 0:\n",
    "            ax.text(\n",
    "                len(stages) - 0.3, y_pos,\n",
    "                f\"abs:{abstain_rate:.0%}\",\n",
    "                ha=\"left\", va=\"center\", fontsize=6, alpha=0.5,\n",
    "            )\n",
    "\n",
    "    ax.set_xticks(range(len(stages)))\n",
    "    ax.set_xticklabels([f\"Stage {s}\" for s in stages], fontsize=9)\n",
    "    ax.set_yticks(range(n_evidence))\n",
    "    ax.set_yticklabels(list(reversed(evidence_order)), fontsize=9)\n",
    "    ax.set_title(f\"{model}\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlim(-0.5, len(stages) - 0.5)\n",
    "    ax.grid(True, alpha=0.1)\n",
    "\n",
    "axes[0].set_ylabel(\"Evidence\")\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Bootstrap closed-world DST: BetP with 95% CI (n_boot={N_BOOT}, abstentions dropped)\\n\"\n",
    "    \"Circle size = BetP, error bars = bootstrap CI, gray rows = high abstain rate\",\n",
    "    fontsize=12,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "X = simplex_wide[betp_cols].values\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "simplex_wide[\"pc1\"] = X_2d[:, 0]\n",
    "simplex_wide[\"pc2\"] = X_2d[:, 1]\n",
    "\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_}\")\n",
    "print(f\"PC1: {pca.explained_variance_ratio_[0]:.1%}, PC2: {pca.explained_variance_ratio_[1]:.1%}\")\n",
    "print(f\"Component loadings:\\n{pd.DataFrame(pca.components_, columns=betp_cols, index=['PC1', 'PC2']).round(3)}\")\n",
    "\n",
    "# Plot\n",
    "n_cols_plot = min(n_models, 3)\n",
    "n_rows_plot = (n_models + n_cols_plot - 1) // n_cols_plot\n",
    "fig, axes = plt.subplots(n_rows_plot, n_cols_plot, figsize=(6 * n_cols_plot, 5 * n_rows_plot), sharex=True, sharey=True)\n",
    "axes_flat = np.atleast_1d(axes).flatten()\n",
    "\n",
    "for idx, tag in enumerate(data.tags):\n",
    "    ax = axes_flat[idx]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = simplex_wide[simplex_wide[\"tag\"] == tag]\n",
    "\n",
    "    # Color by max stage conviction\n",
    "    max_betp = sub[betp_cols].max(axis=1).values\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        sub[\"pc1\"],\n",
    "        sub[\"pc2\"],\n",
    "        c=max_betp,\n",
    "        cmap=\"RdYlBu_r\",\n",
    "        vmin=0.25, vmax=1.0,\n",
    "        s=120,\n",
    "        alpha=0.8,\n",
    "        edgecolors=\"black\",\n",
    "        linewidths=0.5,\n",
    "        zorder=3,\n",
    "    )\n",
    "\n",
    "    # Convex hull\n",
    "    if len(sub) >= 3:\n",
    "        points = sub[[\"pc1\", \"pc2\"]].values\n",
    "        try:\n",
    "            hull = ConvexHull(points)\n",
    "            hull_verts = np.append(hull.vertices, hull.vertices[0])\n",
    "            ax.plot(points[hull_verts, 0], points[hull_verts, 1], \"k-\", alpha=0.3, linewidth=1)\n",
    "            ax.fill(points[hull_verts, 0], points[hull_verts, 1], alpha=0.05, color=\"blue\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Label evidence items\n",
    "    for _, row in sub.iterrows():\n",
    "        ax.annotate(\n",
    "            row[\"evidence\"],\n",
    "            (row[\"pc1\"], row[\"pc2\"]),\n",
    "            textcoords=\"offset points\",\n",
    "            xytext=(5, 5),\n",
    "            fontsize=7,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"{model}\", fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.0%} var)\")\n",
    "    ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.0%} var)\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    ax.axhline(0, color=\"k\", linewidth=0.5, alpha=0.2)\n",
    "    ax.axvline(0, color=\"k\", linewidth=0.5, alpha=0.2)\n",
    "\n",
    "for idx in range(n_models, len(axes_flat)):\n",
    "    axes_flat[idx].set_visible(False)\n",
    "\n",
    "fig.colorbar(scatter, ax=axes_flat[:n_models].tolist(), label=\"Max stage conviction\", pad=0.02)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"PCA projection of pignistic probabilities (TBM)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Convex hull shows the 'footprint' of each model's decision space\")\n",
    "print(\"- Larger hull = more diverse responses across evidence items\")\n",
    "print(\"- Tighter cluster = model treats most evidence similarly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bootstrap stability summary per model ---\n",
    "#\n",
    "# For each model, report:\n",
    "# - Mean CI width across all (evidence, stage) cells\n",
    "# - Max CI width (worst-case instability)\n",
    "# - Mean conflict under bootstrap\n",
    "# - Fraction of cells where CI width > 0.1 (materially unstable)\n",
    "\n",
    "stability_rows = []\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = boot_df[boot_df[\"tag\"] == tag]\n",
    "\n",
    "    ci_widths = sub[\"betP_q975\"] - sub[\"betP_q025\"]\n",
    "\n",
    "    stability_rows.append({\n",
    "        \"model\": model,\n",
    "        \"n_cells\": len(sub),\n",
    "        \"mean_ci_width\": float(ci_widths.mean()),\n",
    "        \"median_ci_width\": float(ci_widths.median()),\n",
    "        \"max_ci_width\": float(ci_widths.max()),\n",
    "        \"frac_unstable\": float((ci_widths > 0.1).mean()),\n",
    "        \"mean_betP_std\": float(sub[\"betP_std\"].mean()),\n",
    "        \"mean_conflict\": float(sub[\"conflict_mean\"].mean()),\n",
    "    })\n",
    "\n",
    "stability_df = pd.DataFrame(stability_rows).sort_values(\"mean_ci_width\")\n",
    "\n",
    "print(\"Bootstrap DST Stability Summary\")\n",
    "print(f\"(n_boot={N_BOOT}, per-evidence TBM combination)\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    stability_df.style.format({\n",
    "        \"mean_ci_width\": \"{:.4f}\",\n",
    "        \"median_ci_width\": \"{:.4f}\",\n",
    "        \"max_ci_width\": \"{:.4f}\",\n",
    "        \"frac_unstable\": \"{:.1%}\",\n",
    "        \"mean_betP_std\": \"{:.4f}\",\n",
    "        \"mean_conflict\": \"{:.3f}\",\n",
    "    }).hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "print(\"\\nMetric definitions:\")\n",
    "print(\"- mean_ci_width: Avg 95% CI width of BetP across all (evidence, stage) cells\")\n",
    "print(\"- max_ci_width: Worst-case CI width (most unstable cell)\")\n",
    "print(\"- frac_unstable: % of cells where CI width > 0.1 (materially sensitive to rubric selection)\")\n",
    "print(\"- mean_conflict: Avg TBM conflict under bootstrap (higher = more contradictory evidence)\")\n",
    "\n",
    "geometry_metrics = []\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = simplex_wide[simplex_wide[\"tag\"] == tag]\n",
    "\n",
    "    if len(sub) < 2:\n",
    "        continue\n",
    "\n",
    "    # 1. Convex hull area in PCA space\n",
    "    points_2d = sub[[\"pc1\", \"pc2\"]].values\n",
    "    try:\n",
    "        hull = ConvexHull(points_2d)\n",
    "        hull_area = hull.volume  # In 2D, .volume gives area\n",
    "    except Exception:\n",
    "        hull_area = 0.0\n",
    "\n",
    "    # 2. Mean pairwise distance in 4D pignistic space (not PCA)\n",
    "    betp_matrix = sub[betp_cols].values\n",
    "    pairwise_dists = pdist(betp_matrix, metric=\"euclidean\")\n",
    "    mean_dist = float(np.mean(pairwise_dists))\n",
    "    std_dist = float(np.std(pairwise_dists))\n",
    "\n",
    "    # 3. Shannon entropy of pignistic distribution per evidence\n",
    "    entropies = []\n",
    "    for vec in betp_matrix:\n",
    "        v = vec + 1e-10\n",
    "        v = v / v.sum()\n",
    "        entropies.append(float(-np.sum(v * np.log2(v))))\n",
    "\n",
    "    mean_entropy = float(np.mean(entropies))\n",
    "    std_entropy = float(np.std(entropies))\n",
    "\n",
    "    # 4. Discreteness score: fraction of evidence items with max(betP) > 0.8\n",
    "    max_betp = betp_matrix.max(axis=1)\n",
    "    discreteness = float((max_betp > 0.8).mean())\n",
    "\n",
    "    geometry_metrics.append({\n",
    "        \"model\": model,\n",
    "        \"n_evidence\": len(sub),\n",
    "        \"hull_area_pca\": hull_area,\n",
    "        \"mean_pairwise_dist\": mean_dist,\n",
    "        \"std_pairwise_dist\": std_dist,\n",
    "        \"mean_entropy\": mean_entropy,\n",
    "        \"std_entropy\": std_entropy,\n",
    "        \"discreteness_score\": discreteness,\n",
    "    })\n",
    "\n",
    "geometry_df = pd.DataFrame(geometry_metrics).sort_values(\"discreteness_score\", ascending=False)\n",
    "\n",
    "print(\"Decision Space Geometry Metrics\")\n",
    "print(\"=\" * 80)\n",
    "display(\n",
    "    geometry_df.style.format({\n",
    "        \"hull_area_pca\": \"{:.4f}\",\n",
    "        \"mean_pairwise_dist\": \"{:.4f}\",\n",
    "        \"std_pairwise_dist\": \"{:.4f}\",\n",
    "        \"mean_entropy\": \"{:.3f}\",\n",
    "        \"std_entropy\": \"{:.3f}\",\n",
    "        \"discreteness_score\": \"{:.1%}\",\n",
    "    }).hide(axis=\"index\")\n",
    "\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "print(\"\\nMetric definitions:\")\n",
    "print(\"- hull_area_pca: Area of convex hull in PCA space (larger = more spread)\")\n",
    "print(\"- mean_pairwise_dist: Avg euclidean distance between evidence BetP vectors (larger = more diverse)\")\n",
    "print(\"- mean_entropy: Avg Shannon entropy of BetP distributions (higher = more uncertain)\")\n",
    "print(\"- discreteness_score: % of evidence items with max(BetP) > 0.8 (higher = more decisive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
