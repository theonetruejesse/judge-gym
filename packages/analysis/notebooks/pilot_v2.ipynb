{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Pilot Study v2\n",
    "\n",
    "Multiple experiments (from `TAGS`) scoring news articles on a 4-point fascism rubric.\n",
    "\n",
    "Data is pulled via `judge_gym.collect.pull_experiments` (single bulk Convex query per experiment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize as mplNormalize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from itertools import combinations\n",
    "from functools import reduce\n",
    "from pyds import MassFunction\n",
    "\n",
    "from judge_gym.collect import pull_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = [\n",
    "    \"ecc-fascism-usa-trial-gpt-4.1\",\n",
    "    \"ecc-fascism-usa-trial-gemini-3.0-flash\",\n",
    "    \"ecc-fascism-usa-trial-gpt-5.2-chat\",\n",
    "    \"ecc-fascism-usa-trial-qwen3-235b\",\n",
    "    \"ecc-fascism-usa-trial-gpt-4.1-mini\",\n",
    "]\n",
    "\n",
    "data = pull_experiments(TAGS)\n",
    "print(f\"Tags pulled: {data.tags}\")\n",
    "print(f\"Scale size:  {data.scale_size}\")\n",
    "print(f\"Scores:      {len(data.scores)} rows\")\n",
    "print(f\"Evidence:    {len(data.evidence)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = data.scores.copy()\n",
    "eid_to_label = dict(zip(data.evidence[\"evidenceId\"], data.evidence[\"label\"]))\n",
    "scores[\"evidence\"] = scores[\"evidenceId\"].map(eid_to_label)\n",
    "\n",
    "data.evidence[[\"label\", \"title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Subset-exploded stage counts per evidence\n",
    "\n",
    "Stacked bar charts showing the distribution of individual stages chosen across all verdicts,\n",
    "plus abstain rates. Each verdict is exploded into its component stages (e.g., `[2,3]` → stage 2 + stage 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Subset-exploded stage counts per evidence ---\n",
    "# Vertical stacked bar charts, one panel per model (2 columns).\n",
    "# Each verdict is exploded into component stages, plus abstain counts.\n",
    "\n",
    "def _explode_stages(row):\n",
    "    \"\"\"Explode a verdict into individual stage selections.\"\"\"\n",
    "    if row[\"abstained\"]:\n",
    "        return [\"ABSTAIN\"]\n",
    "    else:\n",
    "        return row[\"decodedScores\"]\n",
    "\n",
    "# Build exploded stage counts per evidence per model\n",
    "n_models = len(data.tags)\n",
    "n_cols = 2\n",
    "n_rows = (n_models + n_cols - 1) // n_cols  # Ceiling division\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(10 * n_cols, 6 * n_rows), sharey=True)\n",
    "\n",
    "# Flatten axes array for easier iteration\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "# Get all stages + ABSTAIN\n",
    "all_stages = list(range(1, data.scale_size + 1)) + [\"ABSTAIN\"]\n",
    "\n",
    "# Heat scale colors: stage 1 (coolest) to stage 4 (hottest), then grey for ABSTAIN\n",
    "# Using YlOrRd colormap for heat scale\n",
    "heat_colors = plt.cm.YlOrRd(np.linspace(0.3, 0.9, data.scale_size))\n",
    "stage_colors = list(heat_colors) + [(0.7, 0.7, 0.7)]  # Add grey for ABSTAIN\n",
    "\n",
    "for idx, tag in enumerate(data.tags):\n",
    "    ax = axes[idx]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = scores[scores[\"experimentTag\"] == tag].copy()\n",
    "    \n",
    "    # Explode verdicts into individual stages\n",
    "    sub[\"stages\"] = sub.apply(_explode_stages, axis=1)\n",
    "    exploded = sub.explode(\"stages\")\n",
    "    \n",
    "    # Count stages per evidence\n",
    "    ev_labels = sorted(eid_to_label.values(), key=lambda l: int(l[1:]))\n",
    "    stage_counts = (\n",
    "        exploded.groupby([\"evidence\", \"stages\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    \n",
    "    # Reindex to ensure all stages and evidence are present\n",
    "    stage_counts = stage_counts.reindex(index=ev_labels, columns=all_stages, fill_value=0)\n",
    "    \n",
    "    # Create vertical stacked bar chart\n",
    "    stage_counts.plot(\n",
    "        kind=\"bar\",\n",
    "        stacked=True,\n",
    "        ax=ax,\n",
    "        color=stage_colors,\n",
    "        width=0.7,\n",
    "        legend=(idx == n_models - 1),  # Only show legend on last panel\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"{model}\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_xlabel(\"Evidence\")\n",
    "    ax.grid(axis=\"y\", alpha=0.3)\n",
    "    ax.set_xticklabels(ev_labels, rotation=0)\n",
    "    \n",
    "    # Add count annotations on bars\n",
    "    for i, evidence in enumerate(ev_labels):\n",
    "        cumulative = 0\n",
    "        for stage in all_stages:\n",
    "            count = stage_counts.loc[evidence, stage]\n",
    "            if count > 0:\n",
    "                ax.text(\n",
    "                    i,\n",
    "                    cumulative + count / 2,\n",
    "                    str(int(count)),\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=8,\n",
    "                    fontweight=\"bold\",\n",
    "                    color=\"white\" if count > 5 else \"black\",\n",
    "                )\n",
    "                cumulative += count\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "# Adjust legend\n",
    "if n_models > 0:\n",
    "    axes[n_models - 1].legend(\n",
    "        title=\"Stage\",\n",
    "        bbox_to_anchor=(1.05, 1),\n",
    "        loc=\"upper left\",\n",
    "        frameon=True,\n",
    "    )\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Subset-exploded stage counts per evidence\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=0.98,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Rubric stage length bias analysis\n",
    "\n",
    "Following Dubois et al. (2024), we test whether models exhibit systematic bias toward longer\n",
    "or shorter rubric stage descriptions. We run both pooled (across all rubrics) and per-rubric\n",
    "regressions to assess length effects.\n",
    "\n",
    "**Key finding**: Length bias is **highly heterogeneous across rubrics** — some rubrics show\n",
    "strong positive bias (prefer longer stages), others show strong negative bias (prefer shorter),\n",
    "with effects ranging from -0.29 to +0.30. The pooled effect averages to near-zero, masking\n",
    "this variation. Given this heterogeneity, we **do not apply a length discount** — any global\n",
    "correction would be arbitrary and could amplify bias in rubrics where the effect runs opposite\n",
    "to the average.\n",
    "\n",
    "### Notes\n",
    "- Given Multiple testing: With 30 rubrics × 3 models = 90 tests, ~3 will be \"significant\" by chance at p < 0.05. Could use Bonferroni correction (p < 0.05/60 = 0.0008) Or FDR correction (less conservative).\n",
    "- Could use a signficance based adjustment when there's statistical evidence of length bias with per rubric, adjusted by direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rubric stage length bias regression ---\n",
    "\n",
    "# Flatten rubric quality stats if present\n",
    "rubrics = data.rubrics.copy()\n",
    "if rubrics.empty:\n",
    "    raise ValueError(\"No rubrics returned. Check Convex endpoint.\")\n",
    "\n",
    "if \"qualityStats\" in rubrics.columns:\n",
    "    rubrics[\"observabilityScore\"] = rubrics[\"qualityStats\"].apply(\n",
    "        lambda q: q.get(\"observabilityScore\") if isinstance(q, dict) else np.nan\n",
    "    )\n",
    "    rubrics[\"discriminabilityScore\"] = rubrics[\"qualityStats\"].apply(\n",
    "        lambda q: q.get(\"discriminabilityScore\") if isinstance(q, dict) else np.nan\n",
    "    )\n",
    "\n",
    "def _word_count(text: str) -> int:\n",
    "    \"\"\"Count words in a text string.\"\"\"\n",
    "    tokens = re.split(r\"\\s+\", (text or \"\").strip())\n",
    "    return len([t for t in tokens if t])\n",
    "\n",
    "def _stage_text(stage: dict) -> str:\n",
    "    \"\"\"Concatenate stage label + criteria into a single text string.\"\"\"\n",
    "    parts = [stage.get(\"label\", \"\")]\n",
    "    parts.extend(stage.get(\"criteria\", []))\n",
    "    return \" \".join([p for p in parts if p])\n",
    "\n",
    "# Build stage-level table: one row per rubric stage\n",
    "stage_rows = []\n",
    "for _, r in rubrics.iterrows():\n",
    "    stages = r.get(\"stages\") or []\n",
    "    for idx, stage in enumerate(stages, start=1):\n",
    "        stage_rows.append({\n",
    "            \"rubricId\": r[\"rubricId\"],\n",
    "            \"stage\": idx,\n",
    "            \"stage_len\": _word_count(_stage_text(stage)),\n",
    "            \"observabilityScore\": r.get(\"observabilityScore\"),\n",
    "            \"discriminabilityScore\": r.get(\"discriminabilityScore\"),\n",
    "        })\n",
    "\n",
    "stage_df = pd.DataFrame(stage_rows)\n",
    "\n",
    "# Z-score stage length within each rubric\n",
    "stage_df[\"stage_len_z\"] = stage_df.groupby(\"rubricId\")[\"stage_len\"].transform(\n",
    "    lambda s: (s - s.mean()) / s.std(ddof=0) if s.std(ddof=0) > 0 else 0.0\n",
    ")\n",
    "\n",
    "# Add score ID before expanding (so we can group back later)\n",
    "scores[\"scoreId\"] = scores.index\n",
    "\n",
    "# Expand scores to score × stage rows for regression\n",
    "score_stage = scores.merge(stage_df, on=\"rubricId\", how=\"left\")\n",
    "score_stage[\"selected\"] = score_stage.apply(\n",
    "    lambda r: 0 if r[\"abstained\"] or r[\"decodedScores\"] is None\n",
    "    else int(r[\"stage\"] in r[\"decodedScores\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Fit stage-length bias per model (linear probability model)\n",
    "# DV: selected (0/1), IV: stage_len_z + fixed effects for evidence + rubric quality\n",
    "betas = {}\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = score_stage[score_stage[\"experimentTag\"] == tag].copy()\n",
    "    sub = sub[~sub[\"abstained\"]]  # exclude abstains from regression\n",
    "    if sub.empty:\n",
    "        betas[model] = 0.0\n",
    "        continue\n",
    "    formula = \"selected ~ stage_len_z + C(evidence) + observabilityScore + discriminabilityScore\"\n",
    "    res = smf.ols(formula, data=sub).fit()\n",
    "    betas[model] = float(res.params.get(\"stage_len_z\", 0.0))\n",
    "    print(f\"{model}: stage_len_z beta = {betas[model]:.4f} (n={len(sub)}, p={res.pvalues.get('stage_len_z', np.nan):.4f})\")\n",
    "\n",
    "# Rubric quality proxy and final adjusted probe\n",
    "rubric_quality = stage_df.groupby(\"rubricId\")[[\"observabilityScore\", \"discriminabilityScore\"]].first().reset_index()\n",
    "scores = scores.merge(rubric_quality, on=\"rubricId\", how=\"left\")\n",
    "scores[\"p_rubric\"] = scores[\"observabilityScore\"] * scores[\"discriminabilityScore\"]\n",
    "scores[\"p_score\"] = scores[\"expertAgreementProb\"]\n",
    "scores[\"p_adjusted\"] = scores[\"p_score\"] * scores[\"p_rubric\"]  # No length discount\n",
    "\n",
    "# Per-rubric regressions to see heterogeneity in length bias\n",
    "print(\"\\n=== Per-rubric length bias regressions ===\")\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = score_stage[score_stage[\"experimentTag\"] == tag].copy()\n",
    "    sub = sub[~sub[\"abstained\"]]\n",
    "    \n",
    "    rubric_results = []\n",
    "    for rubric_id in sub[\"rubricId\"].unique():\n",
    "        rubric_sub = sub[sub[\"rubricId\"] == rubric_id].copy()\n",
    "        \n",
    "        # Need at least some variation to fit\n",
    "        if rubric_sub[\"selected\"].nunique() < 2 or len(rubric_sub) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Simple regression: selected ~ stage_len_z (no other controls, just length effect)\n",
    "        try:\n",
    "            formula = \"selected ~ stage_len_z\"\n",
    "            res_rubric = smf.ols(formula, data=rubric_sub).fit()\n",
    "            \n",
    "            rubric_results.append({\n",
    "                \"rubricId\": rubric_id,\n",
    "                \"beta\": res_rubric.params.get(\"stage_len_z\", np.nan),\n",
    "                \"pvalue\": res_rubric.pvalues.get(\"stage_len_z\", np.nan),\n",
    "                \"n_obs\": len(rubric_sub),\n",
    "                \"n_selected\": rubric_sub[\"selected\"].sum(),\n",
    "            })\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    rubric_df = pd.DataFrame(rubric_results)\n",
    "    rubric_df[\"selection_rate\"] = rubric_df[\"n_selected\"] / rubric_df[\"n_obs\"]\n",
    "    \n",
    "    \n",
    "    print(f\"\\n--- {model} ---\")\n",
    "    print(f\"Overall beta: {betas[model]:.4f}\")\n",
    "    print(f\"Rubrics analyzed: {len(rubric_df)}\")\n",
    "    print(f\"\\nTop 10 rubrics by absolute beta (strongest length effects):\")\n",
    "    \n",
    "    top_rubrics = rubric_df.sort_values(\"beta\", key=abs, ascending=False).head(10)\n",
    "    display(top_rubrics[[\"rubricId\", \"beta\", \"pvalue\", \"selection_rate\", \"n_obs\"]].style.format({\n",
    "        \"beta\": \"{:.4f}\",\n",
    "        \"pvalue\": \"{:.4f}\",\n",
    "        \"selection_rate\": \"{:.3f}\",\n",
    "        \"n_obs\": \"{:.0f}\",\n",
    "    }).hide(axis=\"index\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Abstain & specificity rates per evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstain and singleton rates (uses scores + eid_to_label from regression cell above)\n",
    "\n",
    "def rates_table(scores: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Per model: abstain % and singleton-commit % for each evidence.\n",
    "\n",
    "    Columns use 'E1 (N=30)' notation so the sample size is in the header.\n",
    "    Rows are multi-indexed: (model, metric).\n",
    "    \"\"\"\n",
    "    labels = sorted(eid_to_label.values(), key=lambda l: int(l[1:]))\n",
    "\n",
    "    records = []\n",
    "    for tag in data.tags:\n",
    "        sub = scores[scores[\"experimentTag\"] == tag]\n",
    "        model = data.experiments[tag][\"modelId\"]\n",
    "        abstain_row = {}\n",
    "        single_row = {}\n",
    "\n",
    "        for label in labels:\n",
    "            g = sub[sub[\"evidence\"] == label]\n",
    "            n = len(g)\n",
    "            col = f\"{label} (N={n})\"\n",
    "            # Abstain rate\n",
    "            a = g[\"abstained\"].sum()\n",
    "            abstain_row[col] = f\"{a / n * 100:.0f}%\" if n else \"—\"\n",
    "            # Singleton rate (of non-abstained)\n",
    "            non_abs = g[~g[\"abstained\"]]\n",
    "            nn = len(non_abs)\n",
    "            s = non_abs[\"decodedScores\"].apply(len).eq(1).sum() if nn else 0\n",
    "            single_row[col] = f\"{s / nn * 100:.0f}%\" if nn else \"—\"\n",
    "\n",
    "        # Total column\n",
    "        n_total = len(sub)\n",
    "        a_total = sub[\"abstained\"].sum()\n",
    "        non_abs_total = sub[~sub[\"abstained\"]]\n",
    "        nn_total = len(non_abs_total)\n",
    "        s_total = non_abs_total[\"decodedScores\"].apply(len).eq(1).sum() if nn_total else 0\n",
    "\n",
    "        abstain_row[f\"Total (N={n_total})\"] = f\"{a_total / n_total * 100:.0f}%\"\n",
    "        single_row[f\"Total (N={n_total})\"] = f\"{s_total / nn_total * 100:.0f}%\" if nn_total else \"—\"\n",
    "\n",
    "        records.append((model, \"Abstain %\", abstain_row))\n",
    "        records.append((model, \"Singleton %\", single_row))\n",
    "\n",
    "    idx = pd.MultiIndex.from_tuples([(r[0], r[1]) for r in records])\n",
    "    return pd.DataFrame([r[2] for r in records], index=idx)\n",
    "\n",
    "rates_table(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Verdict distribution heatmaps with proportions and average expert probe\n",
    "\n",
    "Each cell displays **(proportion, avg expertAgreementProb)** for that evidence-verdict combination,\n",
    "both formatted with 2 decimal places. All responses — **including abstains** — are included. \n",
    "Each row (evidence) is normalized to sum to 1.\n",
    "\n",
    "One panel per model, stacked vertically. Columns are powerset verdicts sorted by\n",
    "center-of-gravity, then cardinality. Empty cells (0.00) are left blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Verdict distribution heatmaps with proportions and average expert probe ---\n",
    "# Two panels per row (2 columns). Evidence on y-axis, powerset verdicts on x-axis.\n",
    "# Each cell shows (proportion [0,1], avg_expertAgreementProb) with 2 sig figs.\n",
    "# Rows are normalized to sum to 1. A thin visual separator is drawn before ABSTAIN.\n",
    "\n",
    "def _col_sort_key(col: str):\n",
    "    \"\"\"Sort verdict columns: by center-of-gravity, then cardinality, then elements.\n",
    "    ABSTAIN always sorts last.\"\"\"\n",
    "    if col == \"ABSTAIN\":\n",
    "        return (999, 0, [])\n",
    "    stages = ast.literal_eval(col)  # e.g. \"[2, 3]\" -> [2, 3]\n",
    "    cog = sum(stages) / len(stages)\n",
    "    return (cog, len(stages), stages)\n",
    "\n",
    "# Collect all verdicts across all models\n",
    "tmp_all = scores.copy()\n",
    "tmp_all[\"verdict\"] = tmp_all.apply(\n",
    "    lambda r: \"ABSTAIN\" if r[\"abstained\"] else str(sorted(r[\"decodedScores\"])),\n",
    "    axis=1,\n",
    ")\n",
    "all_verdicts = sorted(tmp_all[\"verdict\"].unique(), key=_col_sort_key)\n",
    "\n",
    "def _build_count_and_avg_matrices(sub: pd.DataFrame, all_verdicts: list) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Build evidence × verdict count matrix and average expertAgreementProb matrix.\"\"\"\n",
    "    ev_labels = sorted(eid_to_label.values(), key=lambda l: int(l[1:]))\n",
    "    tmp = sub.copy()\n",
    "    tmp[\"verdict\"] = tmp.apply(\n",
    "        lambda r: \"ABSTAIN\" if r[\"abstained\"] else str(sorted(r[\"decodedScores\"])),\n",
    "        axis=1,\n",
    "    )\n",
    "    tmp[\"expertAgreementProb\"] = tmp[\"expertAgreementProb\"].fillna(1.0)\n",
    "    \n",
    "    # Count matrix\n",
    "    count_pivot = (\n",
    "        tmp.groupby([\"evidence\", \"verdict\"])\n",
    "        .size()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=ev_labels, columns=all_verdicts, fill_value=0)\n",
    "    )\n",
    "    \n",
    "    # Average expertAgreementProb matrix\n",
    "    avg_pivot = (\n",
    "        tmp.groupby([\"evidence\", \"verdict\"])[\"expertAgreementProb\"]\n",
    "        .mean()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=ev_labels, columns=all_verdicts, fill_value=0)\n",
    "    )\n",
    "    \n",
    "    return count_pivot, avg_pivot\n",
    "\n",
    "def _insert_thin_separator(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Insert a narrow NaN spacer column before ABSTAIN for a small visual gap.\"\"\"\n",
    "    if \"ABSTAIN\" not in df.columns:\n",
    "        return df\n",
    "    cols = [c for c in df.columns if c != \"ABSTAIN\"]\n",
    "    sep = pd.DataFrame(np.nan, index=df.index, columns=[\" \"])\n",
    "    return pd.concat([df[cols], sep, df[[\"ABSTAIN\"]]], axis=1)\n",
    "\n",
    "n_models = len(data.tags)\n",
    "n_rows = (n_models + 1) // 2  # 2 charts per row\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(34, 6 * n_rows))\n",
    "if n_models == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif n_rows == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "# Flatten axes for easier iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, tag in enumerate(data.tags):\n",
    "    ax = axes_flat[idx]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = scores[scores[\"experimentTag\"] == tag]\n",
    "\n",
    "    count_mat, avg_mat = _build_count_and_avg_matrices(sub, all_verdicts)\n",
    "\n",
    "    # Convert counts to proportions (row-normalized to [0, 1])\n",
    "    prop_mat = count_mat.div(count_mat.sum(axis=1), axis=0)\n",
    "\n",
    "    # Insert thin spacer before ABSTAIN\n",
    "    prop_mat = _insert_thin_separator(prop_mat)\n",
    "    avg_mat = _insert_thin_separator(avg_mat)\n",
    "\n",
    "    mask = prop_mat.isna()\n",
    "    plot_data = prop_mat.fillna(0)\n",
    "\n",
    "    # Use proportion for heatmap color intensity\n",
    "    sns.heatmap(\n",
    "        plot_data,\n",
    "        annot=False,\n",
    "        mask=mask,\n",
    "        cmap=\"YlOrRd\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        linewidths=0.5,\n",
    "        cbar=True,\n",
    "        cbar_kws={\"label\": \"Proportion\"},\n",
    "        ax=ax,\n",
    "    )\n",
    "\n",
    "    # Annotations: (proportion, avg_prob) with 2 sig figs\n",
    "    for row_i in range(plot_data.shape[0]):\n",
    "        for col_j in range(plot_data.shape[1]):\n",
    "            prop = prop_mat.iloc[row_i, col_j]\n",
    "            avg_prob = avg_mat.iloc[row_i, col_j]\n",
    "            if pd.isna(prop) or prop == 0:\n",
    "                continue\n",
    "            \n",
    "            # Format text with 2 sig figs\n",
    "            text = f\"({prop:.2f}, {avg_prob:.2f})\"\n",
    "            \n",
    "            # Alpha based on proportion\n",
    "            norm = mplNormalize(vmin=0, vmax=1)\n",
    "            alpha = max(0.3, norm(prop))\n",
    "            \n",
    "            ax.text(\n",
    "                col_j + 0.5, row_i + 0.5,\n",
    "                text,\n",
    "                ha=\"center\", va=\"center\",\n",
    "                fontsize=8, fontweight=\"bold\",\n",
    "                color=(0, 0, 0, alpha),\n",
    "            )\n",
    "\n",
    "    # Thin white stripe over the spacer column\n",
    "    spacer_idx = None\n",
    "    for ci, c in enumerate(prop_mat.columns):\n",
    "        if c == \" \":\n",
    "            spacer_idx = ci\n",
    "            break\n",
    "    if spacer_idx is not None:\n",
    "        ax.axvline(x=spacer_idx, color=\"white\", linewidth=3, zorder=5)\n",
    "        ax.axvline(x=spacer_idx + 1, color=\"white\", linewidth=3, zorder=5)\n",
    "\n",
    "    ax.set_title(f\"{model}\", fontsize=13, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Evidence\")\n",
    "    ax.set_xlabel(\"Verdict\")\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Hide any unused subplots\n",
    "for idx in range(n_models, len(axes_flat)):\n",
    "    axes_flat[idx].set_visible(False)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Verdict distribution per evidence: (proportion, avg expertAgreementProb)\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    "    y=0.995,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Belief Function Analysis (Transferable Belief Model)\n",
    "\n",
    "We model each LLM response as a **mass function** in the Dempster-Shafer / Smets TBM framework,\n",
    "using an **open-world assumption** where mass on the empty set represents contradiction.\n",
    "\n",
    "**Frame of discernment:** `Theta = {1, 2, ..., scale_size}` (the ordinal rubric stages).\n",
    "\n",
    "**Mass assignment rules** (let `p = expertAgreementProb`):\n",
    "\n",
    "| Response type | m(verdict) | m(Theta) | m({}) |\n",
    "|---|---|---|---|\n",
    "| **Normal verdict** (proper subset, e.g. `{2,3}`) | p | 1 - p | 0 |\n",
    "| **Full frame** (model chose all stages) | -- | p | 1 - p |\n",
    "| **Abstain** (model refused) | -- | 1 - p | p |\n",
    "\n",
    "- **Normal verdict**: standard simple support function. The probe partitions between the specific verdict and ignorance.\n",
    "- **Full frame**: a confident \"could be anything\" is genuine ignorance; an unconfident one is treated as contradiction.\n",
    "- **Abstain**: a confident refusal is genuine contradiction; an unconfident one is closer to ignorance.\n",
    "\n",
    "Full-frame and abstain are **symmetric mirrors** on the ignorance-contradiction axis, with the probe as the pivot.\n",
    "\n",
    "We compute mass functions **per rubric** (each rubric is a stochastic draw from the design space),\n",
    "then extract pignistic probabilities to see what each rubric \"thinks\" about each evidence article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build per-rubric TBM mass functions ---\n",
    "\n",
    "theta = frozenset(range(1, data.scale_size + 1))  # e.g. frozenset({1, 2, 3, 4})\n",
    "\n",
    "def response_to_mass(row: pd.Series, theta: frozenset) -> MassFunction:\n",
    "    \"\"\"\n",
    "    Convert a single model response to a Dempster-Shafer mass function (TBM).\n",
    "\n",
    "    Rules (let p = expertAgreementProb):\n",
    "      - Normal verdict (proper subset of Theta): m(verdict) = p, m(Theta) = 1-p\n",
    "      - Full frame (verdict == Theta):           m(Theta) = p, m({}) = 1-p\n",
    "      - Abstain:                                 m({}) = p,    m(Theta) = 1-p\n",
    "    \"\"\"\n",
    "    p = float(row.get(\"expertAgreementProb\") or 1.0)\n",
    "\n",
    "    if row[\"abstained\"]:\n",
    "        # Abstain: probe -> contradiction, remainder -> ignorance\n",
    "        m = MassFunction()\n",
    "        m[frozenset()] = p        # contradiction\n",
    "        m[theta] = 1.0 - p        # ignorance\n",
    "        return m\n",
    "\n",
    "    verdict = frozenset(int(s) for s in row[\"decodedScores\"])\n",
    "\n",
    "    if verdict == theta:\n",
    "        # Full frame: probe -> ignorance, remainder -> contradiction\n",
    "        m = MassFunction()\n",
    "        m[theta] = p               # genuine ignorance\n",
    "        m[frozenset()] = 1.0 - p   # contradiction\n",
    "        return m\n",
    "\n",
    "    # Normal verdict: simple support function\n",
    "    m = MassFunction()\n",
    "    m[verdict] = p                 # specific verdict\n",
    "    m[theta] = 1.0 - p            # ignorance\n",
    "    return m\n",
    "\n",
    "\n",
    "# Build per-rubric mass functions and extract pignistic probabilities\n",
    "stages = sorted(theta)  # [1, 2, 3, 4]\n",
    "\n",
    "per_rubric_rows = []\n",
    "for _, row in scores.iterrows():\n",
    "    tag = row[\"experimentTag\"]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    ev_label = row[\"evidence\"]\n",
    "    rubric_id = row[\"rubricId\"]\n",
    "\n",
    "    # Build the individual mass function\n",
    "    m = response_to_mass(row, theta)\n",
    "\n",
    "    # Pignistic transformation\n",
    "    pign = m.pignistic()\n",
    "\n",
    "    rec = {\n",
    "        \"model\": model,\n",
    "        \"tag\": tag,\n",
    "        \"evidence\": ev_label,\n",
    "        \"rubricId\": rubric_id,\n",
    "        \"p_score\": row.get(\"expertAgreementProb\", np.nan),\n",
    "        \"abstained\": row[\"abstained\"],\n",
    "        \"verdict\": \"ABSTAIN\" if row[\"abstained\"] else str(sorted(row[\"decodedScores\"])),\n",
    "        \"conflict\": m[frozenset()],\n",
    "    }\n",
    "    for s in stages:\n",
    "        singleton = frozenset({s})\n",
    "        rec[f\"betP_{s}\"] = pign[singleton] if singleton in pign else 0.0\n",
    "\n",
    "    # Max pignistic probability (conviction measure)\n",
    "    rec[\"max_betP\"] = max([rec[f\"betP_{s}\"] for s in stages])\n",
    "    rec[\"modal_stage\"] = max(stages, key=lambda s: rec[f\"betP_{s}\"])\n",
    "\n",
    "    per_rubric_rows.append(rec)\n",
    "\n",
    "    per_rubric_rows.append(rec)\n",
    "\n",
    "per_rubric_df = pd.DataFrame(per_rubric_rows)\n",
    "\n",
    "print(f\"Built {len(per_rubric_df)} per-rubric mass functions\")\n",
    "print(f\"Frame: Theta = {set(theta)}\")\n",
    "\n",
    "\n",
    "# --- Combine mass functions per rubric (aggregate across all evidence) ---\n",
    "\n",
    "combined_results = []\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = scores[scores[\"experimentTag\"] == tag]\n",
    "\n",
    "    # Get all unique rubrics for this experiment\n",
    "    for rubric_id in sub[\"rubricId\"].unique():\n",
    "        rubric_sub = sub[sub[\"rubricId\"] == rubric_id]\n",
    "        if rubric_sub.empty:\n",
    "            continue\n",
    "\n",
    "        # Build mass functions for all evidence scored by this rubric\n",
    "        masses = [response_to_mass(row, theta) for _, row in rubric_sub.iterrows()]\n",
    "\n",
    "        # Combine via unnormalized conjunctive rule (Smets' TBM)\n",
    "        combined = reduce(\n",
    "            lambda a, b: a.combine_conjunctive(b, normalization=False),\n",
    "            masses,\n",
    "        )\n",
    "\n",
    "        # Extract DST metrics\n",
    "        conflict = combined[frozenset()]\n",
    "        \n",
    "        # Handle edge case: if conflict = 1.0, pignistic is undefined\n",
    "        if conflict >= 0.9999:\n",
    "            pign = {}  # No belief to distribute\n",
    "        else:\n",
    "            pign = combined.pignistic()\n",
    "\n",
    "        result = {\n",
    "            \"model\": model,\n",
    "            \"tag\": tag,\n",
    "            \"rubricId\": rubric_id,\n",
    "            \"n_evidence\": len(rubric_sub),\n",
    "            \"conflict\": conflict,\n",
    "        }\n",
    "\n",
    "        for s in stages:\n",
    "            singleton = frozenset({s})\n",
    "            result[f\"bel_{s}\"] = combined.bel(singleton)\n",
    "            result[f\"pl_{s}\"] = combined.pl(singleton)\n",
    "            result[f\"betP_{s}\"] = pign[singleton] if singleton in pign else 0.0\n",
    "\n",
    "        # Max pignistic (conviction)\n",
    "        result[\"max_betP\"] = max([result[f\"betP_{s}\"] for s in stages])\n",
    "        result[\"modal_stage\"] = max(stages, key=lambda s: result[f\"betP_{s}\"])\n",
    "\n",
    "        combined_results.append(result)\n",
    "\n",
    "combined_df = pd.DataFrame(combined_results)\n",
    "\n",
    "print(f\"\\nBuilt {len(combined_df)} combined mass functions (per model × rubric)\")\n",
    "\n",
    "# Filter by conflict threshold\n",
    "CONFLICT_THRESHOLD = 0.9\n",
    "\n",
    "print(f\"\\n=== Conflict filtering (threshold = {CONFLICT_THRESHOLD}) ===\")\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = combined_df[combined_df[\"tag\"] == tag].copy()\n",
    "    n_total = len(sub)\n",
    "    n_usable = (sub[\"conflict\"] < CONFLICT_THRESHOLD).sum()\n",
    "    n_unusable = n_total - n_usable\n",
    "    pct_unusable = (n_unusable / n_total * 100) if n_total > 0 else 0\n",
    "    print(f\"{model}: {n_unusable}/{n_total} unusable ({pct_unusable:.1f}% with conflict ≥ {CONFLICT_THRESHOLD})\")\n",
    "\n",
    "# Display top 10 most convictional rubrics per experiment (filtered)\n",
    "print(f\"\\n=== Top 10 most convictional rubrics per experiment (conflict < {CONFLICT_THRESHOLD}) ===\")\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = combined_df[combined_df[\"tag\"] == tag].copy()\n",
    "    \n",
    "    # Filter by conflict threshold\n",
    "    sub_filtered = sub[sub[\"conflict\"] < CONFLICT_THRESHOLD].copy()\n",
    "    \n",
    "    if sub_filtered.empty:\n",
    "        print(f\"\\n--- {model} ---\")\n",
    "        print(f\"No rubrics with conflict < {CONFLICT_THRESHOLD}\")\n",
    "        continue\n",
    "    \n",
    "    top10 = sub_filtered.nlargest(10, \"max_betP\")\n",
    "    \n",
    "    print(f\"\\n--- {model} ({len(sub_filtered)} usable, showing top {len(top10)}) ---\")\n",
    "    \n",
    "    # Build display table with [Bel,Pl] intervals\n",
    "    disp_rows = []\n",
    "    for _, r in top10.iterrows():\n",
    "        row_data = {\n",
    "            \"rubricId\": r[\"rubricId\"],\n",
    "            \"conflict\": r[\"conflict\"],\n",
    "        }\n",
    "        for s in stages:\n",
    "            row_data[f\"[Bel,Pl]({s})\"] = f\"[{r[f'bel_{s}']:.3f}, {r[f'pl_{s}']:.3f}]\"\n",
    "            row_data[f\"BetP({s})\"] = r[f\"betP_{s}\"]\n",
    "        disp_rows.append(row_data)\n",
    "    \n",
    "    disp_df = pd.DataFrame(disp_rows)\n",
    "    \n",
    "    # Apply styling: bold the winning BetP per row\n",
    "    def highlight_max_betP(row):\n",
    "        betP_cols = [f\"BetP({s})\" for s in stages]\n",
    "        betP_vals = [row[col] for col in betP_cols]\n",
    "        max_val = max(betP_vals)\n",
    "        return [\n",
    "            \"font-weight: bold\" if col in betP_cols and row[col] == max_val else \"\"\n",
    "            for col in row.index\n",
    "        ]\n",
    "    \n",
    "    display(disp_df.style\n",
    "        .format({\n",
    "            \"conflict\": \"{:.3f}\",\n",
    "            **{f\"BetP({s})\": \"{:.3f}\" for s in stages},\n",
    "        })\n",
    "        .apply(highlight_max_betP, axis=1)\n",
    "        .hide(axis=\"index\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Closed-World DST Analysis\n",
    "\n",
    "Alternative analysis using **classical Dempster-Shafer** (closed-world assumption):\n",
    "\n",
    "**Key differences from TBM:**\n",
    "- **Drop abstentions** entirely (no mass on empty set)\n",
    "- **Full-frame responses** treated as pure ignorance: `m(Theta) = 1.0`\n",
    "- **Normalized combination** (Dempster's rule with conflict redistribution)\n",
    "\n",
    "**Mass assignment rules** (let `p = expertAgreementProb`):\n",
    "\n",
    "| Response type | m(verdict) | m(Theta) |\n",
    "|---|---|---|\n",
    "| **Normal verdict** (proper subset) | p | 1 - p |\n",
    "| **Full frame** (all stages) | -- | 1.0 |\n",
    "| **Abstain** | *dropped* | *dropped* |\n",
    "\n",
    "This gives us a \"best-case\" view: what do the rubrics say when we only consider their substantive judgments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Closed-World DST: Build per-rubric mass functions ---\n",
    "\n",
    "def response_to_mass_closed(row: pd.Series, theta: frozenset) -> MassFunction | None:\n",
    "    \"\"\"\n",
    "    Convert a single model response to a classical DST mass function (closed-world).\n",
    "\n",
    "    Rules (let p = expertAgreementProb):\n",
    "      - Normal verdict (proper subset): m(verdict) = p, m(Theta) = 1-p\n",
    "      - Full frame (verdict == Theta):  m(Theta) = 1.0 (pure ignorance)\n",
    "      - Abstain:                        None (dropped)\n",
    "    \"\"\"\n",
    "    # Drop abstentions\n",
    "    if row[\"abstained\"]:\n",
    "        return None\n",
    "\n",
    "    p = float(row.get(\"expertAgreementProb\") or 1.0)\n",
    "    verdict = frozenset(int(s) for s in row[\"decodedScores\"])\n",
    "\n",
    "    # Full frame: pure ignorance\n",
    "    if verdict == theta:\n",
    "        m = MassFunction()\n",
    "        m[theta] = 1.0\n",
    "        return m\n",
    "\n",
    "    # Normal verdict: simple support function\n",
    "    m = MassFunction()\n",
    "    m[verdict] = p\n",
    "    m[theta] = 1.0 - p\n",
    "    return m\n",
    "\n",
    "\n",
    "# Build per-rubric mass functions (closed-world)\n",
    "per_rubric_closed_rows = []\n",
    "\n",
    "for _, row in scores.iterrows():\n",
    "    m = response_to_mass_closed(row, theta)\n",
    "    \n",
    "    # Skip abstentions\n",
    "    if m is None:\n",
    "        continue\n",
    "\n",
    "    tag = row[\"experimentTag\"]\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    ev_label = row[\"evidence\"]\n",
    "    rubric_id = row[\"rubricId\"]\n",
    "\n",
    "    # Pignistic transformation\n",
    "    pign = m.pignistic()\n",
    "\n",
    "    rec = {\n",
    "        \"model\": model,\n",
    "        \"tag\": tag,\n",
    "        \"evidence\": ev_label,\n",
    "        \"rubricId\": rubric_id,\n",
    "        \"p_score\": row.get(\"expertAgreementProb\", np.nan),\n",
    "        \"verdict\": str(sorted(row[\"decodedScores\"])),\n",
    "    }\n",
    "    \n",
    "    for s in stages:\n",
    "        singleton = frozenset({s})\n",
    "        rec[f\"betP_{s}\"] = pign[singleton] if singleton in pign else 0.0\n",
    "\n",
    "    rec[\"max_betP\"] = max([rec[f\"betP_{s}\"] for s in stages])\n",
    "    rec[\"modal_stage\"] = max(stages, key=lambda s: rec[f\"betP_{s}\"])\n",
    "\n",
    "    per_rubric_closed_rows.append(rec)\n",
    "\n",
    "per_rubric_closed_df = pd.DataFrame(per_rubric_closed_rows)\n",
    "\n",
    "print(f\"Built {len(per_rubric_closed_df)} per-rubric mass functions (closed-world)\")\n",
    "print(f\"Dropped {len(scores) - len(per_rubric_closed_df)} abstentions\")\n",
    "print(f\"Frame: Theta = {set(theta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Closed-World DST: Combine per rubric with normalized rule ---\n",
    "\n",
    "combined_closed_results = []\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = scores[scores[\"experimentTag\"] == tag]\n",
    "\n",
    "    for rubric_id in sub[\"rubricId\"].unique():\n",
    "        rubric_sub = sub[sub[\"rubricId\"] == rubric_id]\n",
    "        if rubric_sub.empty:\n",
    "            continue\n",
    "\n",
    "        # Build mass functions, filtering out abstentions\n",
    "        masses = []\n",
    "        for _, row in rubric_sub.iterrows():\n",
    "            m = response_to_mass_closed(row, theta)\n",
    "            if m is not None:\n",
    "                masses.append(m)\n",
    "\n",
    "        # Skip if no valid responses\n",
    "        if not masses:\n",
    "            continue\n",
    "\n",
    "        # First combine unnormalized to get conflict\n",
    "        combined_unnorm = reduce(\n",
    "            lambda a, b: a.combine_conjunctive(b, normalization=False),\n",
    "            masses,\n",
    "        )\n",
    "        conflict = combined_unnorm[frozenset()]\n",
    "\n",
    "        # Then combine normalized (classic Dempster)\n",
    "        combined = reduce(\n",
    "            lambda a, b: a.combine_conjunctive(b, normalization=True),\n",
    "            masses,\n",
    "        )\n",
    "\n",
    "        # Extract DST metrics\n",
    "        pign = combined.pignistic()\n",
    "\n",
    "        result = {\n",
    "            \"model\": model,\n",
    "            \"tag\": tag,\n",
    "            \"rubricId\": rubric_id,\n",
    "            \"n_evidence\": len(masses),  # count non-abstentions\n",
    "            \"conflict\": conflict,  # conflict that was normalized away\n",
    "        }\n",
    "\n",
    "        for s in stages:\n",
    "            singleton = frozenset({s})\n",
    "            result[f\"bel_{s}\"] = combined.bel(singleton)\n",
    "            result[f\"pl_{s}\"] = combined.pl(singleton)\n",
    "            result[f\"betP_{s}\"] = pign[singleton] if singleton in pign else 0.0\n",
    "\n",
    "        result[\"max_betP\"] = max([result[f\"betP_{s}\"] for s in stages])\n",
    "        result[\"modal_stage\"] = max(stages, key=lambda s: result[f\"betP_{s}\"])\n",
    "\n",
    "        combined_closed_results.append(result)\n",
    "\n",
    "combined_closed_df = pd.DataFrame(combined_closed_results)\n",
    "\n",
    "print(f\"\\nBuilt {len(combined_closed_df)} combined mass functions (closed-world, per model × rubric)\")\n",
    "\n",
    "# Filter by conflict threshold (same as TBM)\n",
    "CONFLICT_THRESHOLD_CLOSED = 0.9\n",
    "\n",
    "print(f\"\\n=== Conflict filtering (threshold = {CONFLICT_THRESHOLD_CLOSED}) ===\")\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = combined_closed_df[combined_closed_df[\"tag\"] == tag].copy()\n",
    "    n_total = len(sub)\n",
    "    n_usable = (sub[\"conflict\"] < CONFLICT_THRESHOLD_CLOSED).sum()\n",
    "    n_unusable = n_total - n_usable\n",
    "    pct_unusable = (n_unusable / n_total * 100) if n_total > 0 else 0\n",
    "    print(f\"{model}: {n_unusable}/{n_total} unusable ({pct_unusable:.1f}% with conflict ≥ {CONFLICT_THRESHOLD_CLOSED})\")\n",
    "\n",
    "# Display top 10 most convictional rubrics per experiment (filtered)\n",
    "print(f\"\\n=== Top 10 most convictional rubrics per experiment (closed-world, conflict < {CONFLICT_THRESHOLD_CLOSED}) ===\")\n",
    "\n",
    "for tag in data.tags:\n",
    "    model = data.experiments[tag][\"modelId\"]\n",
    "    sub = combined_closed_df[combined_closed_df[\"tag\"] == tag].copy()\n",
    "    \n",
    "    # Filter by conflict threshold\n",
    "    sub_filtered = sub[sub[\"conflict\"] < CONFLICT_THRESHOLD_CLOSED].copy()\n",
    "    \n",
    "    if sub_filtered.empty:\n",
    "        print(f\"\\n--- {model} ---\")\n",
    "        print(f\"No rubrics with conflict < {CONFLICT_THRESHOLD_CLOSED}\")\n",
    "        continue\n",
    "    \n",
    "    top10 = sub_filtered.nlargest(10, \"max_betP\")\n",
    "    \n",
    "    print(f\"\\n--- {model} ({len(sub_filtered)} usable, showing top {len(top10)}) ---\")\n",
    "    \n",
    "    # Build display table with conflict and BetP values\n",
    "    disp_rows = []\n",
    "    for _, r in top10.iterrows():\n",
    "        row_data = {\n",
    "            \"rubricId\": r[\"rubricId\"],\n",
    "            \"n_evidence\": int(r[\"n_evidence\"]),\n",
    "            \"conflict\": r[\"conflict\"],\n",
    "        }\n",
    "        for s in stages:\n",
    "            row_data[f\"BetP({s})\"] = r[f\"betP_{s}\"]\n",
    "        disp_rows.append(row_data)\n",
    "    \n",
    "    disp_df = pd.DataFrame(disp_rows)\n",
    "    \n",
    "    # Apply styling: bold the winning BetP per row\n",
    "    def highlight_max_betP(row):\n",
    "        betP_cols = [f\"BetP({s})\" for s in stages]\n",
    "        betP_vals = [row[col] for col in betP_cols]\n",
    "        max_val = max(betP_vals)\n",
    "        return [\n",
    "            \"font-weight: bold\" if col in betP_cols and row[col] == max_val else \"\"\n",
    "            for col in row.index\n",
    "        ]\n",
    "    \n",
    "    display(disp_df.style\n",
    "        .format({\n",
    "            **{f\"BetP({s})\": \"{:.3f}\" for s in stages},\n",
    "        })\n",
    "        .apply(highlight_max_betP, axis=1)\n",
    "        .hide(axis=\"index\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final DST aggregation (weighted by rubric critic scores) ---\n",
    "\n",
    "CONFLICT_THRESHOLD_FINAL = 0.9\n",
    "stage_cols = [f\"betP_{s}\" for s in stages]\n",
    "\n",
    "# One quality weight per (tag, rubric)\n",
    "rubric_weights = (\n",
    "    scores.groupby([\"experimentTag\", \"rubricId\"], as_index=False)[\"p_rubric\"]\n",
    "    .mean()\n",
    "    .rename(columns={\"experimentTag\": \"tag\"})\n",
    ")\n",
    "rubric_weights[\"p_rubric\"] = rubric_weights[\"p_rubric\"].fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "\n",
    "def weighted_quantile(values: np.ndarray, weights: np.ndarray, q: float) -> float:\n",
    "    values = np.asarray(values, dtype=float)\n",
    "    weights = np.asarray(weights, dtype=float)\n",
    "\n",
    "    mask = np.isfinite(values) & np.isfinite(weights)\n",
    "    values = values[mask]\n",
    "    weights = weights[mask]\n",
    "\n",
    "    if len(values) == 0:\n",
    "        return np.nan\n",
    "    if weights.sum() <= 0:\n",
    "        return float(np.quantile(values, q))\n",
    "\n",
    "    sorter = np.argsort(values)\n",
    "    values = values[sorter]\n",
    "    weights = weights[sorter]\n",
    "\n",
    "    cdf = np.cumsum(weights)\n",
    "    cdf = cdf / cdf[-1]\n",
    "    return float(np.interp(q, cdf, values))\n",
    "\n",
    "\n",
    "def aggregate_per_evidence(\n",
    "    per_df: pd.DataFrame,\n",
    "    combined_conflict_df: pd.DataFrame,\n",
    "    method_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    usable = (\n",
    "        combined_conflict_df[combined_conflict_df[\"conflict\"] < CONFLICT_THRESHOLD_FINAL][[\"tag\", \"rubricId\"]]\n",
    "        .drop_duplicates()\n",
    "    )\n",
    "\n",
    "    merged = (\n",
    "        per_df.merge(usable, on=[\"tag\", \"rubricId\"], how=\"inner\")\n",
    "        .merge(rubric_weights, on=[\"tag\", \"rubricId\"], how=\"left\")\n",
    "    )\n",
    "    merged[\"p_rubric\"] = merged[\"p_rubric\"].fillna(0.0)\n",
    "\n",
    "    rows = []\n",
    "    group_cols = [\"model\", \"tag\", \"evidence\"]\n",
    "\n",
    "    for (model, tag, evidence), g in merged.groupby(group_cols):\n",
    "        n_rubrics = int(g[\"rubricId\"].nunique())\n",
    "\n",
    "        for s in stages:\n",
    "            values = g[f\"betP_{s}\"].to_numpy(dtype=float)\n",
    "            weights = g[\"p_rubric\"].to_numpy(dtype=float)\n",
    "\n",
    "            if np.nansum(weights) <= 0:\n",
    "                weights = np.ones_like(values, dtype=float)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"method\": method_name,\n",
    "                    \"model\": model,\n",
    "                    \"tag\": tag,\n",
    "                    \"evidence\": evidence,\n",
    "                    \"stage\": s,\n",
    "                    \"mean_betP\": float(np.average(values, weights=weights)),\n",
    "                    \"q10_betP\": weighted_quantile(values, weights, 0.10),\n",
    "                    \"q90_betP\": weighted_quantile(values, weights, 0.90),\n",
    "                    \"n_rubrics\": n_rubrics,\n",
    "                    \"weight_sum\": float(np.nansum(weights)),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    # Keep evidence in E1..E9 order\n",
    "    out[\"evidence_num\"] = out[\"evidence\"].str.extract(r\"E(\\d+)\").astype(float)\n",
    "    out = out.sort_values([\"model\", \"evidence_num\", \"stage\"]).drop(columns=[\"evidence_num\"])\n",
    "    return out\n",
    "\n",
    "\n",
    "agg_tbm = aggregate_per_evidence(per_rubric_df, combined_df, \"TBM\")\n",
    "agg_closed = aggregate_per_evidence(per_rubric_closed_df, combined_closed_df, \"Closed-world\")\n",
    "agg_all = pd.concat([agg_tbm, agg_closed], ignore_index=True)\n",
    "\n",
    "print(\"Built weighted per-evidence DST aggregates\")\n",
    "print(f\"TBM rows: {len(agg_tbm)} | Closed-world rows: {len(agg_closed)}\")\n",
    "print(f\"Conflict threshold: {CONFLICT_THRESHOLD_FINAL}\")\n",
    "\n",
    "# Quick sanity check: stage probabilities should sum to ~1 per (method, model, evidence)\n",
    "check = (\n",
    "    agg_all.groupby([\"method\", \"model\", \"evidence\"], as_index=False)[\"mean_betP\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"mean_betP\": \"sum_mean_betP\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final visualization: weighted mean BetP heatmaps ---\n",
    "\n",
    "methods = [\"TBM\", \"Closed-world\"]\n",
    "models = [data.experiments[tag][\"modelId\"] for tag in data.tags]\n",
    "\n",
    "n_rows = len(methods)\n",
    "n_cols = len(models)\n",
    "fig, axes = plt.subplots(\n",
    "    n_rows,\n",
    "    n_cols,\n",
    "    figsize=(6 * n_cols, 4.8 * n_rows),\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    squeeze=False,\n",
    ")\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    for j, model in enumerate(models):\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        sub = agg_all[(agg_all[\"method\"] == method) & (agg_all[\"model\"] == model)].copy()\n",
    "        pivot = sub.pivot(index=\"evidence\", columns=\"stage\", values=\"mean_betP\")\n",
    "\n",
    "        # Ensure consistent axis order\n",
    "        evidence_order = sorted(pivot.index, key=lambda x: int(re.search(r\"E(\\d+)\", x).group(1)))\n",
    "        pivot = pivot.reindex(index=evidence_order)\n",
    "        pivot = pivot.reindex(columns=stages)\n",
    "\n",
    "        show_cbar = j == (n_cols - 1)\n",
    "        sns.heatmap(\n",
    "            pivot,\n",
    "            ax=ax,\n",
    "            cmap=\"viridis\",\n",
    "            vmin=0.0,\n",
    "            vmax=1.0,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            cbar=show_cbar,\n",
    "            cbar_kws={\"label\": \"Weighted mean BetP\"} if show_cbar else None,\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"{method} | {model}\")\n",
    "        ax.set_xlabel(\"Stage\")\n",
    "        ax.set_ylabel(\"Evidence\")\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Final stage belief per evidence (weighted by rubric critic score, conflict-filtered)\",\n",
    "    y=1.01,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conflict comparison + summary table ---\n",
    "\n",
    "conflict_plot_df = pd.concat(\n",
    "    [\n",
    "        combined_df[[\"model\", \"tag\", \"rubricId\", \"conflict\"]].assign(method=\"TBM\"),\n",
    "        combined_closed_df[[\"model\", \"tag\", \"rubricId\", \"conflict\"]].assign(method=\"Closed-world\"),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "for ax, method in zip(axes, [\"TBM\", \"Closed-world\"]):\n",
    "    sub = conflict_plot_df[conflict_plot_df[\"method\"] == method]\n",
    "    sns.boxplot(data=sub, x=\"model\", y=\"conflict\", ax=ax)\n",
    "    sns.stripplot(data=sub, x=\"model\", y=\"conflict\", ax=ax, color=\"black\", alpha=0.45, size=3)\n",
    "    ax.axhline(CONFLICT_THRESHOLD_FINAL, color=\"red\", linestyle=\"--\", linewidth=1)\n",
    "    ax.set_title(f\"{method} conflict distribution\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Conflict\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Summary table (per method x model)\n",
    "summary_rows = []\n",
    "for method, df in [(\"TBM\", combined_df), (\"Closed-world\", combined_closed_df)]:\n",
    "    for tag in data.tags:\n",
    "        model = data.experiments[tag][\"modelId\"]\n",
    "        sub = df[df[\"tag\"] == tag].copy()\n",
    "\n",
    "        n_total = len(sub)\n",
    "        n_usable = int((sub[\"conflict\"] < CONFLICT_THRESHOLD_FINAL).sum())\n",
    "        usable_pct = (100.0 * n_usable / n_total) if n_total else np.nan\n",
    "\n",
    "        usable = sub[sub[\"conflict\"] < CONFLICT_THRESHOLD_FINAL]\n",
    "        mean_conviction = usable[\"max_betP\"].mean() if not usable.empty else np.nan\n",
    "\n",
    "        summary_rows.append(\n",
    "            {\n",
    "                \"method\": method,\n",
    "                \"model\": model,\n",
    "                \"n_total\": n_total,\n",
    "                \"n_usable\": n_usable,\n",
    "                \"usable_pct\": usable_pct,\n",
    "                \"mean_conflict\": sub[\"conflict\"].mean() if n_total else np.nan,\n",
    "                \"median_conflict\": sub[\"conflict\"].median() if n_total else np.nan,\n",
    "                \"mean_max_betP_usable\": mean_conviction,\n",
    "            }\n",
    "        )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values([\"method\", \"model\"])\n",
    "\n",
    "print(f\"Conflict threshold: {CONFLICT_THRESHOLD_FINAL}\")\n",
    "display(\n",
    "    summary_df.style.format(\n",
    "        {\n",
    "            \"usable_pct\": \"{:.1f}%\",\n",
    "            \"mean_conflict\": \"{:.3f}\",\n",
    "            \"median_conflict\": \"{:.3f}\",\n",
    "            \"mean_max_betP_usable\": \"{:.3f}\",\n",
    "        }\n",
    "    ).hide(axis=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Pairwise Divergence Analysis\n",
    "\n",
    "We compare final per-evidence stage distributions between model pairs using:\n",
    "\n",
    "- **Jensen-Shannon divergence (JSD)** as the primary symmetric metric\n",
    "- **Kullback-Leibler divergence (KL)** in both directions to capture asymmetry\n",
    "- **Total variation (TV)** distance as an interpretable mass-shift measure\n",
    "\n",
    "This is computed for both **TBM** and **Closed-world** aggregated BetP distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute pairwise divergence metrics (JSD, KL, TV) ---\n",
    "\n",
    "\n",
    "def normalize_with_eps(p: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = np.where(np.isfinite(p), p, 0.0)\n",
    "    p = np.clip(p, eps, None)\n",
    "    return p / p.sum()\n",
    "\n",
    "\n",
    "def kl_div(p: np.ndarray, q: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    p = normalize_with_eps(p, eps)\n",
    "    q = normalize_with_eps(q, eps)\n",
    "    return float(np.sum(p * np.log(p / q)))\n",
    "\n",
    "\n",
    "def js_div(p: np.ndarray, q: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    p = normalize_with_eps(p, eps)\n",
    "    q = normalize_with_eps(q, eps)\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_div(p, m, eps) + 0.5 * kl_div(q, m, eps)\n",
    "\n",
    "\n",
    "def tv_dist(p: np.ndarray, q: np.ndarray, eps: float = 1e-8) -> float:\n",
    "    p = normalize_with_eps(p, eps)\n",
    "    q = normalize_with_eps(q, eps)\n",
    "    return float(0.5 * np.sum(np.abs(p - q)))\n",
    "\n",
    "\n",
    "# Build wide table: one row per (method, model, evidence), columns betP_1..betP_4\n",
    "agg_wide = agg_all.pivot_table(\n",
    "    index=[\"method\", \"model\", \"evidence\"],\n",
    "    columns=\"stage\",\n",
    "    values=\"mean_betP\",\n",
    "    aggfunc=\"first\",\n",
    ").reset_index()\n",
    "\n",
    "agg_wide.columns = [\n",
    "    c if isinstance(c, str) else f\"betP_{int(c)}\"\n",
    "    for c in agg_wide.columns\n",
    "]\n",
    "\n",
    "stage_prob_cols = [f\"betP_{s}\" for s in stages]\n",
    "model_order = [data.experiments[tag][\"modelId\"] for tag in data.tags]\n",
    "model_pairs = list(combinations(model_order, 2))\n",
    "\n",
    "rows = []\n",
    "for method in agg_wide[\"method\"].unique():\n",
    "    method_df = agg_wide[agg_wide[\"method\"] == method]\n",
    "    evidences = sorted(\n",
    "        method_df[\"evidence\"].unique(),\n",
    "        key=lambda x: int(re.search(r\"E(\\d+)\", x).group(1)),\n",
    "    )\n",
    "\n",
    "    for evidence in evidences:\n",
    "        e_df = method_df[method_df[\"evidence\"] == evidence]\n",
    "\n",
    "        for model_a, model_b in model_pairs:\n",
    "            a_row = e_df[e_df[\"model\"] == model_a]\n",
    "            b_row = e_df[e_df[\"model\"] == model_b]\n",
    "            if a_row.empty or b_row.empty:\n",
    "                continue\n",
    "\n",
    "            p = a_row.iloc[0][stage_prob_cols].to_numpy(dtype=float)\n",
    "            q = b_row.iloc[0][stage_prob_cols].to_numpy(dtype=float)\n",
    "\n",
    "            delta = normalize_with_eps(p) - normalize_with_eps(q)\n",
    "            pair_label = f\"{model_a} vs {model_b}\"\n",
    "\n",
    "            row = {\n",
    "                \"method\": method,\n",
    "                \"evidence\": evidence,\n",
    "                \"model_a\": model_a,\n",
    "                \"model_b\": model_b,\n",
    "                \"pair\": pair_label,\n",
    "                \"JSD\": js_div(p, q),\n",
    "                \"KL_a_to_b\": kl_div(p, q),\n",
    "                \"KL_b_to_a\": kl_div(q, p),\n",
    "                \"TV\": tv_dist(p, q),\n",
    "            }\n",
    "\n",
    "            for s, d in zip(stages, delta):\n",
    "                row[f\"delta_betP_{s}\"] = float(d)\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "divergence_df = pd.DataFrame(rows)\n",
    "\n",
    "print(f\"Computed divergence rows: {len(divergence_df)}\")\n",
    "print(f\"Model pairs: {len(model_pairs)} | Methods: {divergence_df['method'].nunique()}\")\n",
    "\n",
    "# Summary by pair/method\n",
    "pair_summary = (\n",
    "    divergence_df.groupby([\"method\", \"pair\"], as_index=False)\n",
    "    .agg(\n",
    "        mean_JSD=(\"JSD\", \"mean\"),\n",
    "        median_JSD=(\"JSD\", \"median\"),\n",
    "        mean_TV=(\"TV\", \"mean\"),\n",
    "        mean_KL_a_to_b=(\"KL_a_to_b\", \"mean\"),\n",
    "        mean_KL_b_to_a=(\"KL_b_to_a\", \"mean\"),\n",
    "        n_evidence=(\"evidence\", \"nunique\"),\n",
    "    )\n",
    "    .sort_values([\"method\", \"mean_JSD\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "display(\n",
    "    pair_summary.style.format(\n",
    "        {\n",
    "            \"mean_JSD\": \"{:.4f}\",\n",
    "            \"median_JSD\": \"{:.4f}\",\n",
    "            \"mean_TV\": \"{:.4f}\",\n",
    "            \"mean_KL_a_to_b\": \"{:.4f}\",\n",
    "            \"mean_KL_b_to_a\": \"{:.4f}\",\n",
    "        }\n",
    "    ).hide(axis=\"index\")\n",
    ")\n",
    "\n",
    "# Top divergent evidence per pair/method\n",
    "top_divergent = (\n",
    "    divergence_df.sort_values([\"method\", \"pair\", \"JSD\"], ascending=[True, True, False])\n",
    "    .groupby([\"method\", \"pair\"], as_index=False)\n",
    "    .head(3)\n",
    ")\n",
    "\n",
    "print(\"Top-3 evidence items by JSD (per method x pair):\")\n",
    "display(\n",
    "    top_divergent[[\"method\", \"pair\", \"evidence\", \"JSD\", \"TV\", \"KL_a_to_b\", \"KL_b_to_a\"]]\n",
    "    .style.format(\n",
    "        {\n",
    "            \"JSD\": \"{:.4f}\",\n",
    "            \"TV\": \"{:.4f}\",\n",
    "            \"KL_a_to_b\": \"{:.4f}\",\n",
    "            \"KL_b_to_a\": \"{:.4f}\",\n",
    "        }\n",
    "    )\n",
    "    .hide(axis=\"index\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
